{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69861670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class ISVGRanker(ABC):\n",
    "    @abstractmethod\n",
    "    def process(svg_list: list[str], prompt: str = None):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a713e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "import cairosvg\n",
    "from PIL import Image\n",
    "\n",
    "def svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an SVG string to a PNG image using CairoSVG.\n",
    "\n",
    "    If the SVG does not define a `viewBox`, it will add one using the provided size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    svg_code : str\n",
    "         The SVG string to convert.\n",
    "    size : tuple[int, int], default=(384, 384)\n",
    "         The desired size of the output PNG image (width, height).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "         The generated PNG image.\n",
    "    \"\"\"\n",
    "    # Ensure SVG has proper size attributes\n",
    "    if \"viewBox\" not in svg_code:\n",
    "        svg_code = svg_code.replace(\"<svg\", f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
    "\n",
    "    # Convert SVG to PNG\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode(\"utf-8\"))\n",
    "    return Image.open(io.BytesIO(png_data)).convert(\"RGB\").resize(size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ed8a484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import cairosvg\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "class ImageProcessor:\n",
    "    def __init__(self, image: Image.Image, seed=None):\n",
    "        \"\"\"Initialize with either a path to an image or a PIL Image object.\"\"\"\n",
    "        self.image = image\n",
    "        self.original_image = self.image.copy()\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.RandomState(seed)\n",
    "        else:\n",
    "            self.rng = np.random\n",
    "\n",
    "    def reset(self):\n",
    "        self.image = self.original_image.copy()\n",
    "        return self\n",
    "\n",
    "    def visualize_comparison(\n",
    "        self,\n",
    "        original_name=\"Original\",\n",
    "        processed_name=\"Processed\",\n",
    "        figsize=(10, 5),\n",
    "        show=True,\n",
    "    ):\n",
    "        \"\"\"Display original and processed images side by side.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "        ax1.imshow(np.asarray(self.original_image))\n",
    "        ax1.set_title(original_name)\n",
    "        ax1.axis(\"off\")\n",
    "\n",
    "        ax2.imshow(np.asarray(self.image))\n",
    "        ax2.set_title(processed_name)\n",
    "        ax2.axis(\"off\")\n",
    "\n",
    "        title = f\"{original_name} vs {processed_name}\"\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout()\n",
    "        if show:\n",
    "            plt.show()\n",
    "        return fig\n",
    "\n",
    "    def apply_median_filter(self, size=3):\n",
    "        \"\"\"Apply median filter to remove outlier pixel values.\n",
    "\n",
    "        Args:\n",
    "             size: Size of the median filter window.\n",
    "        \"\"\"\n",
    "        self.image = self.image.filter(ImageFilter.MedianFilter(size=size))\n",
    "        return self\n",
    "\n",
    "    def apply_bilateral_filter(self, d=9, sigma_color=75, sigma_space=75):\n",
    "        \"\"\"Apply bilateral filter to smooth while preserving edges.\n",
    "\n",
    "        Args:\n",
    "             d: Diameter of each pixel neighborhood\n",
    "             sigma_color: Filter sigma in the color space\n",
    "             sigma_space: Filter sigma in the coordinate space\n",
    "        \"\"\"\n",
    "        # Convert PIL Image to numpy array for OpenCV\n",
    "        img_array = np.asarray(self.image)\n",
    "\n",
    "        # Apply bilateral filter\n",
    "        filtered = cv2.bilateralFilter(img_array, d, sigma_color, sigma_space)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        self.image = Image.fromarray(filtered)\n",
    "        return self\n",
    "\n",
    "    def apply_fft_low_pass(self, cutoff_frequency=0.5):\n",
    "        \"\"\"Apply low-pass filter in the frequency domain using FFT.\n",
    "\n",
    "        Args:\n",
    "             cutoff_frequency: Normalized cutoff frequency (0-1).\n",
    "                  Lower values remove more high frequencies.\n",
    "        \"\"\"\n",
    "        # Convert to numpy array, ensuring float32 for FFT\n",
    "        img_array = np.array(self.image, dtype=np.float32)\n",
    "\n",
    "        # Process each color channel separately\n",
    "        result = np.zeros_like(img_array)\n",
    "        for i in range(3):  # For RGB channels\n",
    "            # Apply FFT\n",
    "            f = np.fft.fft2(img_array[:, :, i])\n",
    "            fshift = np.fft.fftshift(f)\n",
    "\n",
    "            # Create a low-pass filter mask\n",
    "            rows, cols = img_array[:, :, i].shape\n",
    "            crow, ccol = rows // 2, cols // 2\n",
    "            mask = np.zeros((rows, cols), np.float32)\n",
    "            r = int(min(crow, ccol) * cutoff_frequency)\n",
    "            center = [crow, ccol]\n",
    "            x, y = np.ogrid[:rows, :cols]\n",
    "            mask_area = (x - center[0]) ** 2 + (y - center[1]) ** 2 <= r * r\n",
    "            mask[mask_area] = 1\n",
    "\n",
    "            # Apply mask and inverse FFT\n",
    "            fshift_filtered = fshift * mask\n",
    "            f_ishift = np.fft.ifftshift(fshift_filtered)\n",
    "            img_back = np.fft.ifft2(f_ishift)\n",
    "            img_back = np.real(img_back)\n",
    "\n",
    "            result[:, :, i] = img_back\n",
    "\n",
    "        # Clip to 0-255 range and convert to uint8 after processing all channels\n",
    "        result = np.clip(result, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Convert back to PIL Image\n",
    "        self.image = Image.fromarray(result)\n",
    "        return self\n",
    "\n",
    "    def apply_jpeg_compression(self, quality=85):\n",
    "        \"\"\"Apply JPEG compression.\n",
    "\n",
    "        Args:\n",
    "             quality: JPEG quality (0-95). Lower values increase compression.\n",
    "        \"\"\"\n",
    "        buffer = io.BytesIO()\n",
    "        self.image.save(buffer, format=\"JPEG\", quality=quality)\n",
    "        buffer.seek(0)\n",
    "        self.image = Image.open(buffer)\n",
    "        return self\n",
    "\n",
    "    def apply_random_crop_resize(self, crop_percent=0.05):\n",
    "        \"\"\"Randomly crop and resize back to original dimensions.\n",
    "\n",
    "        Args:\n",
    "             crop_percent: Percentage of image to crop (0-0.4).\n",
    "        \"\"\"\n",
    "        width, height = self.image.size\n",
    "        crop_pixels_w = int(width * crop_percent)\n",
    "        crop_pixels_h = int(height * crop_percent)\n",
    "\n",
    "        left = self.rng.randint(0, crop_pixels_w + 1)\n",
    "        top = self.rng.randint(0, crop_pixels_h + 1)\n",
    "        right = width - self.rng.randint(0, crop_pixels_w + 1)\n",
    "        bottom = height - self.rng.randint(0, crop_pixels_h + 1)\n",
    "\n",
    "        self.image = self.image.crop((left, top, right, bottom))\n",
    "        self.image = self.image.resize((width, height), Image.BILINEAR)\n",
    "        return self\n",
    "\n",
    "    def apply(self):\n",
    "        \"\"\"Apply an ensemble of defenses.\"\"\"\n",
    "        return (\n",
    "            self.apply_random_crop_resize(crop_percent=0.03)\n",
    "            .apply_jpeg_compression(quality=95)\n",
    "            .apply_median_filter(size=9)\n",
    "            .apply_fft_low_pass(cutoff_frequency=0.5)\n",
    "            .apply_bilateral_filter(d=5, sigma_color=75, sigma_space=75)\n",
    "            .apply_jpeg_compression(quality=92)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6cebc99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhndt/.conda/envs/pysvgenius/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from more_itertools import chunked\n",
    "import statistics\n",
    "import torch\n",
    "import math\n",
    "from PIL import Image\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "\n",
    "class PaliGemmaRanker(ISVGRanker):\n",
    "    \"\"\"Evaluates images based on their similarity to a given text description using multiple choice questions.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        self.letters = string.ascii_uppercase\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model_path = \"google/paligemma2-10b-mix-448\"\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            self.model_path, use_fast=True)\n",
    "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            self.model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=self.quantization_config,\n",
    "        ).to(self.device)\n",
    "        self.question = ['Does image portray \"SVG illustration of {}\"?',\n",
    "                         \"Do the objects in the image match the following description: {}?\"]\n",
    "        self.choices = [[\"yes\", \"no\"], [\"yes\", \"no\"]]\n",
    "        self.answers = ['yes', 'yes']\n",
    "\n",
    "    def score(self, questions, choices, answers, image, n=4):\n",
    "        scores = []\n",
    "        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n",
    "        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n",
    "            scores.extend(\n",
    "                self.score_batch(\n",
    "                    image,\n",
    "                    question_batch,\n",
    "                    choice_batch,\n",
    "                    answer_batch,\n",
    "                )\n",
    "            )\n",
    "        return statistics.mean(scores)\n",
    "\n",
    "    def score_batch(\n",
    "        self,\n",
    "        image: Image.Image,\n",
    "        questions: list[str],\n",
    "        choices_list: list[list[str]],\n",
    "        answers: list[str],\n",
    "    ) -> list[float]:\n",
    "        \"\"\"Evaluates the image based on multiple choice questions and answers.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : PIL.Image.Image\n",
    "            The image to evaluate.\n",
    "        questions : list[str]\n",
    "            List of questions about the image.\n",
    "        choices_list : list[list[str]]\n",
    "            List of lists of possible answer choices, corresponding to each question.\n",
    "        answers : list[str]\n",
    "            List of correct answers from the choices, corresponding to each question.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[float]\n",
    "            List of scores (values between 0 and 1) representing the probability of the correct answer for each question.\n",
    "        \"\"\"\n",
    "        prompts = [\n",
    "            self.format_prompt(question, choices)\n",
    "            for question, choices in zip(questions, choices_list, strict=True)\n",
    "        ]\n",
    "        batched_choice_probabilities = self.get_choice_probability(\n",
    "            image, prompts, choices_list\n",
    "        )\n",
    "\n",
    "        scores = []\n",
    "        for i, _ in enumerate(questions):\n",
    "            choice_probabilities = batched_choice_probabilities[i]\n",
    "            answer = answers[i]\n",
    "            answer_probability = 0.0\n",
    "            for choice, prob in choice_probabilities.items():\n",
    "                if choice == answer:\n",
    "                    answer_probability = prob\n",
    "                    break\n",
    "            scores.append(answer_probability)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def format_prompt(self, question: str, choices: list[str]) -> str:\n",
    "        prompt = f\"<image>answer en Question: {question}\\nChoices:\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"{self.letters[i]}. {choice}\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def mask_choices(self, logits, choices_list):\n",
    "        \"\"\"Masks logits for the first token of each choice letter for each question in the batch.\"\"\"\n",
    "        batch_size = logits.shape[0]\n",
    "        masked_logits = torch.full_like(logits, float(\"-inf\"))\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i in range(len(choices)):\n",
    "                letter_token = self.letters[i]\n",
    "\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    \" \" + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                if isinstance(first_token, int):\n",
    "                    masked_logits[batch_idx, first_token] = logits[\n",
    "                        batch_idx, first_token\n",
    "                    ]\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    masked_logits[batch_idx, first_token_with_space] = logits[\n",
    "                        batch_idx, first_token_with_space\n",
    "                    ]\n",
    "\n",
    "        return masked_logits\n",
    "\n",
    "    def get_choice_probability(self, image, prompts, choices_list) -> list[dict]:\n",
    "        inputs = self.processor(\n",
    "            images=[image] * len(prompts),\n",
    "            text=prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            # Logits for the last (predicted) token\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            masked_logits = self.mask_choices(logits, choices_list)\n",
    "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
    "\n",
    "        batched_choice_probabilities = []\n",
    "        for batch_idx in range(len(prompts)):\n",
    "            choice_probabilities = {}\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i, choice in enumerate(choices):\n",
    "                letter_token = self.letters[i]\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    \" \" + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                prob = 0.0\n",
    "                if isinstance(first_token, int):\n",
    "                    prob += probabilities[batch_idx, first_token].item()\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    prob += probabilities[batch_idx,\n",
    "                                          first_token_with_space].item()\n",
    "                choice_probabilities[choice] = prob\n",
    "\n",
    "            # Renormalize probabilities for each question\n",
    "            total_prob = sum(choice_probabilities.values())\n",
    "            if total_prob > 0:\n",
    "                renormalized_probabilities = {\n",
    "                    choice: prob / total_prob\n",
    "                    for choice, prob in choice_probabilities.items()\n",
    "                }\n",
    "            else:\n",
    "                renormalized_probabilities = (\n",
    "                    choice_probabilities  # Avoid division by zero if total_prob is 0\n",
    "                )\n",
    "            batched_choice_probabilities.append(renormalized_probabilities)\n",
    "\n",
    "        return batched_choice_probabilities\n",
    "\n",
    "    def ocr(self, image, free_chars=4, use_num_char=False):\n",
    "        inputs = (\n",
    "            self.processor(\n",
    "                text=\"<image>ocr\\n\",\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            .to(torch.float16)\n",
    "            .to(self.model.device)\n",
    "        )\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs, max_new_tokens=32, do_sample=False)\n",
    "            outputs = outputs[0][input_len:]\n",
    "            decoded = self.processor.decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "        num_char = len(decoded)\n",
    "\n",
    "        # Exponentially decreasing towards 0.0 if more than free_chars detected\n",
    "        # ---------------Modified Output----------------------\n",
    "        return (\n",
    "            min(1.0, math.exp(-num_char + free_chars))\n",
    "            if not use_num_char\n",
    "            else (min(1.0, math.exp(-num_char + free_chars)), decoded)\n",
    "        )\n",
    "\n",
    "    def process(self, svg_list: list[str], prompt: str = None):\n",
    "        results = []\n",
    "\n",
    "        question = [template.format(prompt) for template in self.question]\n",
    "        choices = self.choices\n",
    "        answers = self.answers\n",
    "\n",
    "        for svg in svg_list:\n",
    "            image_processor = ImageProcessor(\n",
    "                image=svg_to_png(svg), seed=42).apply()\n",
    "            image = image_processor.image.copy()\n",
    "            score = self.score(\n",
    "                questions=question,\n",
    "                choices=choices,\n",
    "                answers=answers,\n",
    "                image=image\n",
    "            )\n",
    "            results.append({\n",
    "                \"svg\": svg,\n",
    "                \"score\": score\n",
    "            })\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25af9bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:56<00:00, 14.06s/it]\n"
     ]
    }
   ],
   "source": [
    "paligemma_ranker = PaliGemmaRanker()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3570efc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_svg_files_from_folder(folder_path: str) -> list[str]:\n",
    "    svg_contents = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.lower().endswith(\".svg\"):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                svg_contents.append(f.read())\n",
    "\n",
    "    return svg_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2060f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "svg_folder = \"../data/test\"\n",
    "svg_list = load_svg_files_from_folder(svg_folder)\n",
    "prompt = \"a purple forest at dusk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a68ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = paligemma_ranker.process([svg_list[0]], prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "050a48ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'svg': '<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"768\" height=\"768\" viewBox=\"0 0 384 384\"><rect width=\"384\" height=\"384\" fill=\"#92769a\"/><polygon points=\"31.0,0.0 1.0,383.0 154.0,383.0 167.0,190.0 119.0,70.0 99.0,288.0 107.0,45.0 52.0,209.0\" fill=\"#b297b9\"/><polygon points=\"252.0,6.0 247.0,232.0 190.0,233.0 184.0,105.0 247.0,366.0 383.0,383.0 350.0,0.0 318.0,319.0 270.0,317.0\" fill=\"#b297b9\"/><polygon points=\"99.0,0.0 187.0,272.0 195.0,0.0 171.0,70.0 152.0,15.0 169.0,154.0\" fill=\"#050f1f\"/><polygon points=\"252.0,0.0 197.0,0.0 184.0,104.0 232.0,66.0\" fill=\"#f7dbe7\"/><polygon points=\"256.0,0.0 272.0,316.0 318.0,317.0 332.0,4.0 299.0,195.0 318.0,0.0 300.0,60.0 278.0,5.0 286.0,188.0\" fill=\"#4b0d55\"/><polygon points=\"108.0,45.0 101.0,287.0 114.0,287.0\" fill=\"#4b0d55\"/><polygon points=\"205.0,254.0 160.0,320.0 156.0,383.0 211.0,382.0 224.0,286.0 213.0,383.0 229.0,363.0 224.0,383.0 282.0,383.0 238.0,378.0 246.0,320.0\" fill=\"#1f2034\"/><polygon points=\"351.0,0.0 383.0,341.0 383.0,0.0 371.0,67.0\" fill=\"#4b0d55\"/><polygon points=\"261.0,0.0 286.0,185.0 289.0,80.0 278.0,0.0\" fill=\"#b297b9\"/><polygon points=\"325.0,0.0 301.0,85.0 300.0,192.0 323.0,103.0\" fill=\"#b297b9\"/><polygon points=\"29.0,0.0 16.0,176.0 2.0,116.0 13.0,267.0\" fill=\"#4b0d55\"/><polygon points=\"131.0,0.0 151.0,102.0 168.0,110.0 150.0,14.0 170.0,67.0 170.0,0.0\" fill=\"#f7dbe7\"/><polygon points=\"2.0,0.0 1.0,106.0 14.0,173.0 19.0,0.0\" fill=\"#b297b9\"/><polygon points=\"59.0,0.0 53.0,206.0 70.0,207.0 64.0,75.0 81.0,33.0 65.0,53.0\" fill=\"#1f2034\"/><polygon points=\"219.0,78.0 218.0,223.0\" fill=\"#050f1f\"/><polygon points=\"154.0,111.0 168.0,150.0 169.0,110.0\" fill=\"#b297b9\"/><polygon points=\"358.0,0.0 358.0,34.0 370.0,64.0 370.0,0.0\" fill=\"#b297b9\"/><polygon points=\"1.0,0.0 0.0,383.0\" fill=\"#f7dbe7\"/><polygon points=\"244.0,231.0 233.0,223.0 219.0,223.0 193.0,232.0\" fill=\"#1f2034\"/><polygon points=\"312.0,0.0 301.0,0.0 300.0,57.0 309.0,34.0\" fill=\"#b297b9\"/><polygon points=\"82.0,0.0 68.0,0.0 67.0,49.0 78.0,23.0\" fill=\"#b297b9\"/><polygon points=\"122.0,0.0 121.0,55.0 132.0,69.0\" fill=\"#b297b9\"/><polygon points=\"113.0,0.0 112.0,36.0 119.0,52.0 120.0,0.0\" fill=\"#4b0d55\"/><polygon points=\"286.0,0.0 283.0,25.0 290.0,56.0 292.0,0.0\" fill=\"#b297b9\"/><polygon points=\"189.0,0.0 182.0,0.0 182.0,45.0\" fill=\"#f7dbe7\"/><polygon points=\"88.0,0.0 83.0,2.0 77.0,30.0 82.0,33.0\" fill=\"#4b0d55\"/><polygon points=\"240.0,377.0 258.0,378.0 263.0,375.0 263.0,371.0 257.0,366.0 250.0,366.0 246.0,368.0\" fill=\"#7c5c88\"/><polygon points=\"105.0,0.0 110.0,28.0 111.0,0.0\" fill=\"#b297b9\"/><polygon points=\"234.0,345.0 234.0,347.0 235.0,347.0 237.0,349.0 240.0,349.0 241.0,350.0 243.0,350.0 241.0,346.0 239.0,345.0\" fill=\"#7617a6\"/><g><circle cx=\"33.6\" cy=\"30.4\" r=\"9.1\" fill=\"#1f2034\" /><circle cx=\"33.6\" cy=\"30.4\" r=\"7.3\" fill=\"#b297b9\" /><circle cx=\"33.6\" cy=\"30.4\" r=\"4.7\" fill=\"#1f2034\" /></g><!-- Circle bytes: outer=53, middle=53, inner=53, total=159, colors: dark=#1f2034, light=#b297b9, position: top-left --></svg>',\n",
       "  'score': 0.9998211674336436}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb24918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysvgenius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
