{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca864c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09f41f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf21357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhndt/.conda/envs/pysvgenius/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModel\n",
    "path_siglip = \"google/siglip-so400m-patch14-384\"\n",
    "\n",
    "model_siglip     = AutoModel.from_pretrained(path_siglip).to(device)\n",
    "processor_siglip = AutoProcessor.from_pretrained(path_siglip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b5fc531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_images(prompt: str, image_list, batch_size: int = 4):\n",
    "    \"\"\"\n",
    "    prompt       : Text instruction that the images should follow\n",
    "    image_list   : List of image files\n",
    "    batch_size   : Adjust according to GPU memory\n",
    "    return       : (Path of the image with the highest score, list of scores for each image)\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "\n",
    "    for i in range(0, len(image_list), batch_size):\n",
    "        images = image_list[i : i + batch_size]\n",
    "        texts = [prompt]\n",
    "\n",
    "        inputs = processor_siglip(\n",
    "            text=texts,\n",
    "            images=images,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model_siglip(**inputs).logits_per_image  # shape (B, 1)\n",
    "\n",
    "        probs = torch.sigmoid(logits).squeeze(1).cpu()  # Convert to [0, 1] range\n",
    "        scores.extend(probs.tolist())\n",
    "\n",
    "    best_idx = int(torch.tensor(scores).argmax())\n",
    "    return best_idx, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38d863b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia.filters as K\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "from diff_jpeg import diff_jpeg_coding\n",
    "from torch import fft, nn\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "DEFAULT_IMAGE_SIZE = 384\n",
    "DEFAULT_FFT_CUTOFF = 0.5\n",
    "DEFAULT_CROP_PERCENT = 0.05\n",
    "FORWARD_CROP_PERCENT = 0.03\n",
    "FORWARD_JPEG_QUALITY_1 = 95\n",
    "FORWARD_MEDIAN_SIZE = 9\n",
    "FORWARD_FFT_CUTOFF = 0.5\n",
    "FORWARD_BILATERAL_D = 5\n",
    "FORWARD_BILATERAL_SIGMA_COLOR = 75\n",
    "FORWARD_BILATERAL_SIGMA_SPACE = 75\n",
    "FORWARD_JPEG_QUALITY_2 = 92\n",
    "TEST_JPEG_QUALITY = 85\n",
    "COMPARISON_ATOL = 1e-2\n",
    "\n",
    "\n",
    "class ImageProcessorTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    A differentiable image processor using PyTorch for frequency-based filtering.\n",
    "\n",
    "    Args:\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        super().__init__()\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            self.rng = np.random.RandomState(seed)\n",
    "        else:\n",
    "            self.rng = np.random\n",
    "\n",
    "    def apply_fft_low_pass(\n",
    "        self, images: torch.Tensor, cutoff_frequency: float = DEFAULT_FFT_CUTOFF\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a low-pass filter in the frequency domain using FFT (Fast Fourier Transform).\n",
    "\n",
    "        This method removes high-frequency details (edges, noise) from the image while preserving\n",
    "        low-frequency components (smooth areas, large structures). Useful for smoothing or preparing\n",
    "        images for aesthetic analysis.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): A batch of input images with shape (B, C, H, W).\n",
    "                                   Pixel values must be in [0, 1] and images must be square.\n",
    "            cutoff_frequency (float, optional): Normalized cutoff frequency (0-1). Lower values\n",
    "                                                remove more high-frequency content.\n",
    "                                                Defaults to DEFAULT_FFT_CUTOFF.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The filtered images with the same shape (B, C, H, W), clamped to [0, 1].\n",
    "        \"\"\"\n",
    "        b, c, h, w = images.shape\n",
    "        assert h == w, \"The images must be square\"\n",
    "        rows, cols = h, w\n",
    "        crow, ccol = rows // 2, cols // 2\n",
    "\n",
    "        # FFT and center shift\n",
    "        fshift = fft.fftshift(\n",
    "            fft.fft2(images.to(dtype=torch.float32), dim=(-2, -1)), dim=(-2, -1)\n",
    "        )\n",
    "\n",
    "        # Scale cutoff to match current image resolution\n",
    "        cutoff_frequency = cutoff_frequency * h / DEFAULT_IMAGE_SIZE\n",
    "        r = min(crow, ccol) * cutoff_frequency\n",
    "        r_sq = r**2\n",
    "\n",
    "        # Create circular low-pass mask\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.arange(rows, device=images.device),\n",
    "            torch.arange(cols, device=images.device),\n",
    "            indexing=\"ij\",\n",
    "        )\n",
    "        center_dist_sq = (y - crow) ** 2 + (x - ccol) ** 2\n",
    "        mask = (\n",
    "            (center_dist_sq <= r_sq).to(torch.float16).unsqueeze(0).unsqueeze(0)\n",
    "        )  # (1, 1, H, W)\n",
    "\n",
    "        # Apply mask and inverse FFT\n",
    "        fshift_filtered = fshift * mask\n",
    "        f_ishift = fft.ifftshift(fshift_filtered, dim=(-2, -1))\n",
    "        img_back = fft.ifft2(f_ishift, dim=(-2, -1))\n",
    "        img_back_real = torch.real(img_back)\n",
    "\n",
    "        # Clamp values to [0, 1]\n",
    "        filtered_images = torch.clamp(img_back_real, 0, 1)\n",
    "\n",
    "        return filtered_images\n",
    "\n",
    "    def apply_random_crop_resize(\n",
    "        self,\n",
    "        images: torch.Tensor,\n",
    "        crop_percent: float = DEFAULT_CROP_PERCENT,\n",
    "        interpolation=InterpolationMode.BILINEAR,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply random cropping followed by resizing to the original size for a batch of images.\n",
    "\n",
    "        This is commonly used for data augmentation to improve generalization.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Batch of input images with shape (B, C, H, W), values in [0, 1].\n",
    "            crop_percent (float): Max percentage of height/width to randomly crop (0-1).\n",
    "            interpolation: Interpolation method for resizing. Default is bilinear.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Batch of processed images with the same shape (B, C, H, W).\n",
    "        \"\"\"\n",
    "        b, c, h, w = images.shape\n",
    "        original_size = (h, w)\n",
    "\n",
    "        crop_pixels_h = int(h * crop_percent)\n",
    "        crop_pixels_w = int(w * crop_percent)\n",
    "\n",
    "        cropped_images = []\n",
    "\n",
    "        for i in range(b):\n",
    "            left = self.rng.randint(0, crop_pixels_w + 1)\n",
    "            top = self.rng.randint(0, crop_pixels_h + 1)\n",
    "            right = w - self.rng.randint(0, crop_pixels_w + 1)\n",
    "            bottom = h - self.rng.randint(0, crop_pixels_h + 1)\n",
    "            new_w = right - left\n",
    "            new_h = bottom - top\n",
    "            # print(f\"[PyTorch] left: {left}, top: {top}, right: {right}, bottom: {bottom}\")\n",
    "\n",
    "            # Crop & Resize (`TF.resized_crop` is differentiable)\n",
    "            cropped_img = TF.resized_crop(\n",
    "                images[i],\n",
    "                top,\n",
    "                left,\n",
    "                new_h,\n",
    "                new_w,\n",
    "                original_size,\n",
    "                interpolation=interpolation,\n",
    "            )\n",
    "            cropped_images.append(cropped_img)\n",
    "        return torch.stack(cropped_images)\n",
    "\n",
    "    def apply_median_filter(self, images: torch.Tensor, size: int = 3) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a median filter to a batch of images using unfolding for efficiency.\n",
    "\n",
    "        This method avoids using heavy convolutional operations and performs\n",
    "        an exact median blur that is fully differentiable.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input image tensor of shape (B, C, H, W), where\n",
    "                                B = batch size, C = number of channels, H = height, W = width.\n",
    "            size (int): Size of the median filter kernel (must be odd). Default is 3.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Median-filtered image tensor of shape (B, C, H, W).\n",
    "        \"\"\"\n",
    "        # Validate that kernel size is a positive integer\n",
    "        if not isinstance(size, int) or size < 1:\n",
    "            raise ValueError(\"kernel_size must be a positive integer.\")\n",
    "\n",
    "        # Ensure kernel size is odd (median needs a center)\n",
    "        if size % 2 == 0:\n",
    "            size += 1\n",
    "\n",
    "        # Calculate padding size\n",
    "        pad = (size - 1) // 2\n",
    "\n",
    "        # Pad the image using replicate mode to handle borders\n",
    "        x = F.pad(images, (pad, pad, pad, pad), mode=\"replicate\")\n",
    "\n",
    "        # Extract image patches of shape (B, C * size^2, H * W)\n",
    "        patches = F.unfold(x, kernel_size=size)\n",
    "\n",
    "        # Get dimensions\n",
    "        B, CP, HW = patches.shape\n",
    "\n",
    "        # Reshape to (B, C, size*size, H*W) to compute median across kernel window\n",
    "        patches = patches.view(B, images.shape[1], size * size, HW)\n",
    "\n",
    "        # Compute median along the kernel dimension (dim=2)\n",
    "        med = torch.median(patches, dim=2).values\n",
    "\n",
    "        # Reshape result to original image shape\n",
    "        med = med.view_as(images)\n",
    "\n",
    "        return med  # Return the median filtered images\n",
    "\n",
    "    def apply_bilateral_filter(\n",
    "        self, images: torch.Tensor, d=9, sigma_color=75, sigma_space=75\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a bilateral filter using Kornia (differentiable and GPU-friendly).\n",
    "\n",
    "        Bilateral filtering smooths the image while preserving edges, by considering\n",
    "        both spatial closeness and color similarity.\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input image tensor of shape (B, C, H, W),\n",
    "                                with pixel values assumed to be in [0, 1].\n",
    "            d (int, optional): Diameter of the filter kernel. Will be scaled based on image size. Defaults to 9.\n",
    "            sigma_color (float, optional): Filter sigma in the color space (range 0–255). Controls how dissimilar colors are smoothed. Defaults to 75.\n",
    "            sigma_space (float, optional): Filter sigma in the coordinate space. Controls how far pixels influence each other spatially. Defaults to 75.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Bilateral-filtered image tensor of shape (B, C, H, W).\n",
    "        \"\"\"\n",
    "        # Normalize sigma_color from 0–255 scale to [0, 1] for Kornia\n",
    "        normalized_sigma_color = sigma_color / 255.0\n",
    "\n",
    "        # Extract image shape\n",
    "        b, c, h, w = images.shape\n",
    "\n",
    "        # Check if the input is a square image (required by this implementation)\n",
    "        assert h == w, \"The images must be square\"\n",
    "\n",
    "        # Scale the kernel size d proportionally to the input size (relative to a default reference size)\n",
    "        d = int(np.ceil(d * h / DEFAULT_IMAGE_SIZE))\n",
    "\n",
    "        # Apply bilateral filter using Kornia\n",
    "        return K.bilateral_blur(\n",
    "            images,\n",
    "            kernel_size=d,\n",
    "            sigma_color=normalized_sigma_color,\n",
    "            sigma_space=(sigma_space, sigma_space),  # horizontal and vertical\n",
    "        )\n",
    "\n",
    "    def apply_jpeg_compression(self, images: torch.Tensor, quality=85) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Simulate JPEG compression using diff_jpeg (optionally non-differentiable).\n",
    "\n",
    "        Note:\n",
    "            This function uses diff_jpeg to simulate the effect of lossy JPEG compression.\n",
    "            When `ste=False`, the compression is not differentiable (i.e., gradients won't flow).\n",
    "\n",
    "        Args:\n",
    "            images (torch.Tensor): Input image tensor of shape (B, C, H, W),\n",
    "                                with pixel values expected to be in the [0, 1] range.\n",
    "            quality (int, optional): JPEG quality level (1–100); higher means better quality and less compression.\n",
    "                                    Defaults to 85.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: JPEG-compressed (simulated) image tensor of shape (B, C, H, W),\n",
    "                        with pixel values in the [0, 1] range.\n",
    "        \"\"\"\n",
    "        # Get batch size and image dimensions\n",
    "        b, c, h, w = images.shape\n",
    "\n",
    "        # Create a tensor of JPEG quality values for each image in the batch\n",
    "        quality_tensor = torch.tensor(\n",
    "            # Repeat the quality value for each image\n",
    "            [quality] * b,\n",
    "            # Place it on the same device as the images (CPU/GPU)\n",
    "            device=images.device,\n",
    "            dtype=torch.float16,  # Use float16 for memory efficiency\n",
    "        )\n",
    "\n",
    "        # Perform JPEG compression simulation using diff_jpeg\n",
    "        # Inputs are scaled to [0, 255] as JPEG encoding expects that range\n",
    "        # `ste=False` disables straight-through estimator, making this step non-differentiable\n",
    "        compressed_images = diff_jpeg_coding(\n",
    "            images * 255.0, jpeg_quality=quality_tensor, ste=False\n",
    "        )\n",
    "\n",
    "        # Rescale the result back to [0, 1] before returning\n",
    "        return compressed_images / 255.0\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: torch.Tensor,\n",
    "        skip_fft_low_pass=False,\n",
    "        skip_random_crop_resize=False,\n",
    "        skip_median_filter=False,\n",
    "        skip_bilateral_filter=False,\n",
    "        skip_jpeg_compression=False,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply a sequence of image degradation operations to the input tensor.\n",
    "\n",
    "        This method simulates real-world image corruption by applying various\n",
    "        differentiable and non-differentiable image transformations. These\n",
    "        operations can help models learn more robust or aesthetically-aware\n",
    "        representations. Each step can be skipped independently via its\n",
    "        corresponding flag.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor of shape (B, C, H, W) with pixel values in [0, 1].\n",
    "            skip_fft_low_pass (bool): If True, skip FFT low-pass filtering.\n",
    "            skip_random_crop_resize (bool): If True, skip random cropping and resizing.\n",
    "            skip_median_filter (bool): If True, skip median filtering.\n",
    "            skip_bilateral_filter (bool): If True, skip bilateral filtering.\n",
    "            skip_jpeg_compression (bool): If True, skip JPEG compression (both passes).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output image tensor after transformations, same shape as input.\n",
    "        \"\"\"\n",
    "        # 1. Apply random crop and resize (differentiable)\n",
    "        if not skip_random_crop_resize:\n",
    "            x = self.apply_random_crop_resize(\n",
    "                x, crop_percent=FORWARD_CROP_PERCENT\n",
    "            )\n",
    "\n",
    "        # 2. Apply JPEG compression (1st pass - non-differentiable)\n",
    "        if not skip_jpeg_compression:\n",
    "            x = self.apply_jpeg_compression(x, quality=FORWARD_JPEG_QUALITY_1)\n",
    "\n",
    "        # 3. Apply median filtering (partially differentiable)\n",
    "        if not skip_median_filter:\n",
    "            x = self.apply_median_filter(x, size=FORWARD_MEDIAN_SIZE)\n",
    "\n",
    "        # 4. Apply FFT low-pass filter (differentiable)\n",
    "        if not skip_fft_low_pass:\n",
    "            x = self.apply_fft_low_pass(x, cutoff_frequency=FORWARD_FFT_CUTOFF)\n",
    "\n",
    "        # 5. Apply bilateral filter (differentiable)\n",
    "        if not skip_bilateral_filter:\n",
    "            x = self.apply_bilateral_filter(\n",
    "                x,\n",
    "                d=FORWARD_BILATERAL_D,\n",
    "                sigma_color=FORWARD_BILATERAL_SIGMA_COLOR,\n",
    "                sigma_space=FORWARD_BILATERAL_SIGMA_SPACE,\n",
    "            )\n",
    "\n",
    "        # 6. Apply JPEG compression again (2nd pass - non-differentiable)\n",
    "        if not skip_jpeg_compression:\n",
    "            x = self.apply_jpeg_compression(x, quality=FORWARD_JPEG_QUALITY_2)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def apply(self, x: torch.Tensor, **kwargs) -> torch.Tensor:\n",
    "        return self(x, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aab650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class AestheticPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 128),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class AestheticEvaluatorOriginal:\n",
    "    def __init__(self):\n",
    "        self.model_path = \"/home/anhndt/pysvgenius/models/sac+logos+ava1-l14-linearMSE.pth\"\n",
    "        self.clip_model_path = \"/home/anhndt/pysvgenius/models/ViT-L-14.pt\"\n",
    "        self.predictor, self.clip_model, self.preprocessor = self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Loads the aesthetic predictor model and CLIP model.\"\"\"\n",
    "        state_dict = torch.load(self.model_path, weights_only=True, map_location=device)\n",
    "\n",
    "        # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "        predictor = AestheticPredictor(768)\n",
    "        predictor.load_state_dict(state_dict)\n",
    "        predictor.to(device)\n",
    "        predictor.eval()\n",
    "        clip_model, preprocessor = clip.load(self.clip_model_path, device=device)\n",
    "\n",
    "        for param in predictor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        return predictor, clip_model, preprocessor\n",
    "\n",
    "    def score(self, image: Image.Image) -> float:\n",
    "        \"\"\"Predicts the CLIP aesthetic score of an image.\"\"\"\n",
    "        image = self.preprocessor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            # l2 normalize\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            image_features = image_features.cpu().detach().numpy()\n",
    "\n",
    "        score = self.predictor(torch.from_numpy(image_features).to(device).float())\n",
    "\n",
    "        return score.item() / 10.0  # scale to [0, 1]\n",
    "\n",
    "\n",
    "\n",
    "class AestheticEvaluatorTorch:\n",
    "    def __init__(self):\n",
    "        self.model_path = \"/home/anhndt/pysvgenius/models/sac+logos+ava1-l14-linearMSE.pth\"\n",
    "        self.clip_model_path = \"/home/anhndt/pysvgenius/models/ViT-L-14.pt\"\n",
    "        self.predictor, self.clip_model, self.preprocessor = self.load()\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Loads the aesthetic predictor model and CLIP model.\"\"\"\n",
    "        state_dict = torch.load(self.model_path, weights_only=True, map_location=device)\n",
    "\n",
    "        # CLIP embedding dim is 768 for CLIP ViT L 14\n",
    "        predictor = AestheticPredictor(768).half()\n",
    "        predictor.load_state_dict(state_dict)\n",
    "        predictor.to(device)\n",
    "        predictor.eval()\n",
    "        clip_model, _ = clip.load(self.clip_model_path, device=device)\n",
    "        preprocessor = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(224, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.CenterCrop(224),\n",
    "                # transforms.Lambda(lambda x: x.clamp_(0, 1)),\n",
    "                transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return predictor, clip_model, preprocessor\n",
    "\n",
    "    def score(self, image: torch.Tensor, no_grad: bool = False) -> float:\n",
    "        \"\"\"Predicts the CLIP aesthetic score of an image.\"\"\"\n",
    "        if image.ndim != 4:\n",
    "            raise ValueError(f\"image must be 4 channels (shape: {image.shape})\")\n",
    "\n",
    "        with torch.no_grad() if no_grad else contextlib.nullcontext():\n",
    "            image = self.preprocessor(image)\n",
    "            image_features = self.clip_model.encode_image(image)\n",
    "            # l2 normalize\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "            score_tensor = self.predictor(image_features)\n",
    "\n",
    "        return score_tensor / 10.0  # scale to [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40442732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Aesthetic Evaluators...\n",
      "Aesthetic Evaluators loaded.\n",
      "Loading SIGLIP model from: google/siglip-so400m-patch14-384...\n",
      "Using SIGLIP model at: google/siglip-so400m-patch14-384\n",
      "SIGLIP model loaded to cuda with dtype torch.float16.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "def load_aesthetic_evaluators() -> tuple[AestheticEvaluatorTorch]:\n",
    "    print(\"Loading Aesthetic Evaluators...\")\n",
    "    aesthetic_evaluator_torch = AestheticEvaluatorTorch()  # target_size is left as default (224)\n",
    "    print(\"Aesthetic Evaluators loaded.\")\n",
    "    return aesthetic_evaluator_torch\n",
    "\n",
    "\n",
    "def load_siglip_model(\n",
    "    device, dtype, model_path_or_name\n",
    ") -> tuple[AutoModel, AutoProcessor]:\n",
    "    \"\"\"Load the SIGLIP model and processor\"\"\"\n",
    "    print(f\"Loading SIGLIP model from: {model_path_or_name}...\")\n",
    "    resolved_model_path = \"google/siglip-so400m-patch14-384\"\n",
    "    print(f\"Using SIGLIP model at: {resolved_model_path}\")\n",
    "\n",
    "    # SIGLIP models are typically loaded in float32.\n",
    "    # You can specify dtype, but be aware of compatibility.\n",
    "    # For MPS (Mac), float16 might not be supported, so we fallback to float32.\n",
    "    effective_dtype = dtype\n",
    "    if device == \"mps\" and dtype == torch.float16:\n",
    "        print(\"Warning: MPS device selected with float16 for SIGLIP. Forcing float32 for stability.\")\n",
    "        effective_dtype = torch.float32\n",
    "\n",
    "    # Specify torch_dtype at loading and move to device with .to(device)\n",
    "    model = AutoModel.from_pretrained(resolved_model_path, torch_dtype=effective_dtype).to(device)\n",
    "    # processor = AutoProcessor.from_pretrained(resolved_model_path)\n",
    "\n",
    "    model.eval()\n",
    "    print(f\"SIGLIP model loaded to {device} with dtype {model.dtype}.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load models\n",
    "aesthetic_evaluator_torch = load_aesthetic_evaluators()\n",
    "siglip_model = load_siglip_model(\n",
    "    device, torch.float16, model_path_or_name=\"google/siglip-so400m-patch14-384\"\n",
    ")\n",
    "\n",
    "# Clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ccaf313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import trange\n",
    "import numpy as np\n",
    "\n",
    "# import clip # CLIP is not needed\n",
    "import torch\n",
    "import pandas as pd  # For loading text prompts\n",
    "import pydiffvg\n",
    "import torch.nn.functional as F  # For image resizing\n",
    "\n",
    "# Import PIL.Image\n",
    "from PIL import Image\n",
    "\n",
    "# Add imports related to PaliGemma\n",
    "from transformers import (\n",
    "    AutoModel,  # Added for SIGLIP\n",
    "    AutoProcessor,\n",
    ")\n",
    "\n",
    "\n",
    "def render(canvas_width, canvas_height, shapes, shape_groups, seed=0):\n",
    "    \"\"\"Render the scene using pydiffvg\"\"\"\n",
    "    _render = pydiffvg.RenderFunction.apply\n",
    "    scene_args = pydiffvg.RenderFunction.serialize_scene(canvas_width, canvas_height, shapes, shape_groups)\n",
    "    img = _render(\n",
    "        canvas_width,      # width\n",
    "        canvas_height,     # height\n",
    "        2,                 # num_samples_x\n",
    "        2,                 # num_samples_y\n",
    "        seed,              # seed\n",
    "        None,              # background_image\n",
    "        *scene_args,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "# Cosine Annealing with Warmup learning rate scheduler\n",
    "def get_lr(initial_lr, final_lr, iteration, warmup_iterations, total_iterations):\n",
    "    if iteration < warmup_iterations:\n",
    "        return initial_lr * (iteration + 1) / warmup_iterations\n",
    "    progress = (iteration - warmup_iterations) / (total_iterations - warmup_iterations)\n",
    "    return final_lr + (initial_lr - final_lr) * 0.5 * (1 + np.cos(np.pi * progress))\n",
    "\n",
    "\n",
    "# --- Helper function (split out from optimize_single_svg) ---\n",
    "\n",
    "def _calculate_target_image_embedding(\n",
    "    image: Image, model: AutoModel, device: str, dtype: torch.dtype\n",
    ") -> torch.Tensor | None:\n",
    "    \"\"\"Load the target image and compute SIGLIP image embedding\"\"\"\n",
    "\n",
    "    image_size = model.config.vision_config.image_size\n",
    "    target_img_pil = image.convert(\"RGB\").resize((image_size, image_size))\n",
    "    \n",
    "    # Using processor is more robust\n",
    "    # inputs = processor(images=target_img_pil, return_tensors=\"pt\").to(device=device, dtype=dtype)\n",
    "    # pixel_values = inputs[\"pixel_values\"]\n",
    "    \n",
    "    target_img_torch = (\n",
    "        torch.from_numpy(np.array(target_img_pil)).permute(2, 0, 1).unsqueeze(0) / 255.0 * 2.0\n",
    "    ) - 1.0\n",
    "\n",
    "    # Fast version\n",
    "    pixel_values = target_img_torch.to(device=device, dtype=dtype)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        target_embedding = model.get_image_features(pixel_values=pixel_values)\n",
    "    target_embedding.requires_grad_(False)  # Gradients not needed\n",
    "    print(f\"  Computed encoding for target image. Shape: {target_embedding.shape}\")\n",
    "    return target_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af9863ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_svg_and_prepare_params(svg_path: Path) -> tuple[int, int, list, list, dict] | None:\n",
    "    \"\"\"Load an SVG file and collect parameters for optimization\"\"\"\n",
    "    if not svg_path.exists():\n",
    "        print(f\"  Warning: SVG file not found: {svg_path}\")\n",
    "        return None\n",
    "    try:\n",
    "        canvas_width, canvas_height, shapes, shape_groups = pydiffvg.svg_to_scene(str(svg_path))\n",
    "\n",
    "        # Resize the canvas to 384x384\n",
    "        assert canvas_width == canvas_height, \"Image must be square\"\n",
    "        ratio = 384 / canvas_width\n",
    "        canvas_width = 384\n",
    "        canvas_height = 384\n",
    "        for shape in shapes:\n",
    "            if hasattr(shape, \"points\") and shape.points is not None:\n",
    "                shape.points = shape.points * ratio\n",
    "            if hasattr(shape, \"center\") and shape.center is not None:\n",
    "                shape.center = shape.center * ratio\n",
    "            if hasattr(shape, \"radius\") and shape.radius is not None:\n",
    "                shape.radius = shape.radius * ratio\n",
    "            if hasattr(shape, \"p_min\") and shape.p_min is not None:\n",
    "                shape.p_min = shape.p_min * ratio\n",
    "            if hasattr(shape, \"p_max\") and shape.p_max is not None:\n",
    "                shape.p_max = shape.p_max * ratio\n",
    "            if hasattr(shape, \"stroke_width\") and shape.stroke_width is not None:\n",
    "                shape.stroke_width = shape.stroke_width * ratio\n",
    "\n",
    "        shape_groups = [g for g in shape_groups if g.shape_ids.numel() > 0]\n",
    "        print(f\"  Loaded SVG file. Canvas: {canvas_width}x{canvas_height}\")\n",
    "\n",
    "        points_vars = []\n",
    "        stroke_width_vars = []\n",
    "        color_vars = []\n",
    "\n",
    "        # Extract parameters to be optimized from all but the background and overlay elements\n",
    "        for shape in shapes[1:-2]:\n",
    "            if hasattr(shape, \"points\") and shape.points is not None:\n",
    "                shape.points.requires_grad = True\n",
    "                points_vars.append(shape.points)\n",
    "            if hasattr(shape, \"center\") and shape.center is not None:  # Circle\n",
    "                shape.center.requires_grad = True\n",
    "                points_vars.append(shape.center)\n",
    "            if hasattr(shape, \"radius\") and shape.radius is not None:  # Circle\n",
    "                shape.radius.requires_grad = True\n",
    "                points_vars.append(shape.radius)\n",
    "            if hasattr(shape, \"p_min\") and shape.p_min is not None:  # Rect\n",
    "                shape.p_min.requires_grad = True\n",
    "                points_vars.append(shape.p_min)\n",
    "            if hasattr(shape, \"p_max\") and shape.p_max is not None:  # Rect\n",
    "                shape.p_max.requires_grad = True\n",
    "                points_vars.append(shape.p_max)\n",
    "            if hasattr(shape, \"stroke_width\") and shape.stroke_width is not None:\n",
    "                shape.stroke_width.requires_grad = True\n",
    "                stroke_width_vars.append(shape.stroke_width)\n",
    "\n",
    "        for group in shape_groups[:-2]:\n",
    "            if hasattr(group, \"fill_color\") and group.fill_color is not None:\n",
    "                group.fill_color.requires_grad = True\n",
    "                color_vars.append(group.fill_color)\n",
    "            if hasattr(group, \"stroke_color\") and group.stroke_color is not None:\n",
    "                group.stroke_color.requires_grad = True\n",
    "                color_vars.append(group.stroke_color)\n",
    "\n",
    "        print(\n",
    "            f\"  Parameters to optimize: points={len(points_vars)}, stroke_width={len(stroke_width_vars)}, color={len(color_vars)}\"\n",
    "        )\n",
    "        if not points_vars and not stroke_width_vars and not color_vars:\n",
    "            print(\"  Warning: No optimizable parameters found.\")\n",
    "            return None\n",
    "\n",
    "        params = {\n",
    "            \"points_vars\": points_vars,\n",
    "            \"stroke_width_vars\": stroke_width_vars,\n",
    "            \"color_vars\": color_vars,\n",
    "        }\n",
    "        return canvas_width, canvas_height, shapes, shape_groups, params\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error occurred while loading SVG file: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49a34a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setup_optimizer(args, params: dict) -> tuple[torch.optim.Optimizer, list[dict]] | None:\n",
    "    \"\"\"Set up the optimizer and parameter groups\"\"\"\n",
    "    coords_initial_lr = args.lr_points\n",
    "    coords_final_lr = args.lr_points / 10\n",
    "    color_initial_lr = args.lr_color\n",
    "    color_final_lr = args.lr_color / 10\n",
    "    stroke_initial_lr = 0  # Learning rate for stroke width (can be configurable via args)\n",
    "    stroke_final_lr = 0\n",
    "\n",
    "    param_groups = []\n",
    "    if params[\"points_vars\"]:\n",
    "        param_groups.append(\n",
    "            {\n",
    "                \"params\": params[\"points_vars\"],\n",
    "                \"lr\": coords_initial_lr,\n",
    "                \"name\": \"coords\",\n",
    "                \"initial_lr\": coords_initial_lr,\n",
    "                \"final_lr\": coords_final_lr,\n",
    "            }\n",
    "        )\n",
    "    if params[\"stroke_width_vars\"]:\n",
    "        param_groups.append(\n",
    "            {\n",
    "                \"params\": params[\"stroke_width_vars\"],\n",
    "                \"lr\": stroke_initial_lr,\n",
    "                \"name\": \"stroke\",\n",
    "                \"initial_lr\": stroke_initial_lr,\n",
    "                \"final_lr\": stroke_final_lr,\n",
    "            }\n",
    "        )\n",
    "    if params[\"color_vars\"]:\n",
    "        param_groups.append(\n",
    "            {\n",
    "                \"params\": params[\"color_vars\"],\n",
    "                \"lr\": color_initial_lr,\n",
    "                \"name\": \"color\",\n",
    "                \"initial_lr\": color_initial_lr,\n",
    "                \"final_lr\": color_final_lr,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if not param_groups:\n",
    "        print(\"  Error: No valid parameter groups found.\")\n",
    "        return None\n",
    "\n",
    "    optimizer = torch.optim.Adam(param_groups)\n",
    "    return optimizer, param_groups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "647540f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_optimization_loop(\n",
    "    args,\n",
    "    canvas_width,\n",
    "    canvas_height,\n",
    "    shapes,\n",
    "    shape_groups,\n",
    "    optimizer,\n",
    "    param_groups,\n",
    "    aesthetic_evaluator_torch,\n",
    "    similarity_mode,  # モードを追加\n",
    "    siglip_model=None,  # SIGLIPモデルを追加\n",
    "    siglip_processor=None,  # SIGLIPプロセッサを追加\n",
    "    text_prompt=None,  # テキストプロンプトを追加\n",
    "    target_image_embedding=None,  # SIGLIP用ターゲットエンべディング\n",
    "    device=\"cuda\",\n",
    "    dtype=torch.float16,\n",
    "    image=None\n",
    "):\n",
    "    \"\"\"Run optimization loop and return best results\"\"\"\n",
    "    print(\n",
    "        f\"  Starting optimization to minimize loss (Aesthetic + {similarity_mode.upper()}) (Batch Size: {args.batch_size})...\"\n",
    "    )\n",
    "    start_time = time.time()\n",
    "    best_loss = float(\"inf\")\n",
    "\n",
    "    best_iteration = 0\n",
    "    warmup_iterations = args.warmup_iter\n",
    "    batch_size = args.batch_size\n",
    "\n",
    "    image = image.resize((canvas_width, canvas_height))\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    tensor_image = to_tensor(image)\n",
    "    tensor_image = tensor_image.to(device)\n",
    "\n",
    "    seed = int(time.time() * 1000) % (2**32 - 1)\n",
    "    image_processor_torch = ImageProcessorTorch(seed=seed).to(device)\n",
    "    image_processor_torch_ref = ImageProcessorTorch(seed=seed).to(device)\n",
    "\n",
    "    \n",
    "    for t in trange(args.iterations):\n",
    "        # pydiffvg.save_svg(f\"{t}.svg\", canvas_width, canvas_height, shapes, shape_groups)\n",
    "        \n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            group_initial_lr = param_group[\"initial_lr\"]\n",
    "            group_final_lr = param_group[\"final_lr\"]\n",
    "            current_lr = get_lr(group_initial_lr, group_final_lr, t, warmup_iterations, args.iterations)\n",
    "            param_group[\"lr\"] = current_lr\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        img_render_single = (\n",
    "            render(canvas_width, canvas_height, shapes, shape_groups, seed=t)[:, :, :3]\n",
    "            .permute(2, 0, 1)\n",
    "            .unsqueeze(0)\n",
    "            .to(device=device, dtype=dtype)  # dtype\n",
    "        )\n",
    "        img_render_batch = img_render_single.repeat(batch_size, 1, 1, 1)  # (B, C, H_svg, W_svg)\n",
    "\n",
    "        # # Resize to 224x224\n",
    "        # img_render_batch = F.interpolate(img_render_batch, size=(224, 224), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        # Apply ImageProcessorTorch (assumed to perform random cropping and resizing internally)\n",
    "        # Ensure that the output size from ImageProcessorTorch matches the expected input size of PaliGemma\n",
    "        # Here we assume they match, or we will resize later if needed\n",
    "\n",
    "        if t < args.jpeg_iter:\n",
    "            processed_img_render_batch = image_processor_torch.apply(img_render_batch, skip_jpeg_compression=True)\n",
    "        else:\n",
    "            processed_img_render_batch = image_processor_torch.apply(img_render_batch)  # (B, C, H_proc, W_proc), [0, 1]\n",
    "        \n",
    "        # --- Similarity loss computation ---\n",
    "        similarity_loss_raw = torch.tensor(np.inf, device=device, dtype=dtype)\n",
    "        similarity_loss = torch.tensor(0.0, device=device, dtype=dtype)\n",
    "\n",
    "        # color mse\n",
    "        tensor_image_batch = tensor_image.repeat(batch_size, 1, 1, 1)\n",
    "        tensor_image_cropped_batch = image_processor_torch_ref.apply_random_crop_resize(\n",
    "            tensor_image_batch, crop_percent=FORWARD_CROP_PERCENT\n",
    "        )\n",
    "        mse_loss = ((processed_img_render_batch - tensor_image_cropped_batch) ** 2).mean()\n",
    "\n",
    "        try:\n",
    "            # (B, C, H, W), [0,1] -> (B, C, H, W), [-1, 1]\n",
    "            # The SIGLIP processor performs normalization internally, so you can either pass [0,1] or manually match the model's expected input.\n",
    "            # Here we follow exp028 and manually normalize to [-1, 1].\n",
    "            render_pixel_values_batch = (processed_img_render_batch * 2.0) - 1.0\n",
    "            render_pixel_values_batch = render_pixel_values_batch.to(\n",
    "                dtype=siglip_model.dtype\n",
    "            )  # Match the dtype of the SIGLIP model\n",
    "\n",
    "            # Resize to the input size expected by siglip_model\n",
    "            render_pixel_values_batch = F.interpolate(\n",
    "                render_pixel_values_batch,\n",
    "                size=siglip_model.config.vision_config.image_size,\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            rendered_image_embedding_batch = siglip_model.get_image_features(\n",
    "                pixel_values=render_pixel_values_batch\n",
    "            )  # (B, embed_dim)\n",
    "\n",
    "            cosine_similarity_batch = torch.nn.functional.cosine_similarity(\n",
    "                target_image_embedding, rendered_image_embedding_batch, dim=-1\n",
    "            )  # (B,)\n",
    "            similarity_loss_raw = -cosine_similarity_batch.mean()  # Negative sign for loss\n",
    "            similarity_loss = similarity_loss_raw  # Normalization may not be necessary for SIGLIP, needs further consideration\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: Exception occurred during SIGLIP similarity calculation (iter {t}): {e}\")\n",
    "            break\n",
    "\n",
    "        # --- Aesthetic score calculation (batch) ---\n",
    "        try:\n",
    "            # Pass processed_img_render_batch to the aesthetic_evaluator_torch\n",
    "            # The resolution can be different from PaliGemma (AestheticEvaluatorTorch handles resizing internally)\n",
    "            # processed_img_render_batch is in shape BCHW, range [0,1], with the main dtype\n",
    "            aesthetic_score_batch = aesthetic_evaluator_torch.score(\n",
    "                processed_img_render_batch.to(device, dtype=torch.float16)\n",
    "            )  # (B,)\n",
    "            aesthetic_loss = -aesthetic_score_batch.mean()\n",
    "        except Exception as e:\n",
    "            print(f\" Error: Failed to compute aesthetic score (iter {t}): {e}\")\n",
    "            break\n",
    "\n",
    "        # --- Total loss calculation (batch average) ---\n",
    "        if t >= args.aesthetic_iter:\n",
    "            if similarity_mode == \"paligemma\":\n",
    "                loss = args.w_aesthetic * aesthetic_loss + args.w_paligemma * similarity_loss + args.w_mse * mse_loss\n",
    "            elif similarity_mode == \"siglip\":\n",
    "                loss = args.w_aesthetic * aesthetic_loss + args.w_siglip * similarity_loss + args.w_mse * mse_loss\n",
    "            else:  # Should not happen\n",
    "                loss = args.w_aesthetic * aesthetic_loss + args.w_mse * mse_loss\n",
    "        else:  # During aesthetic_iter, use only aesthetic loss\n",
    "            loss = args.w_aesthetic * aesthetic_loss\n",
    "\n",
    "        if torch.isnan(loss):\n",
    "            print(f\"  Total loss became NaN (iter {t}). Stopping optimization.\")\n",
    "            break\n",
    "\n",
    "\n",
    "        try:\n",
    "            loss.backward()\n",
    "            with torch.no_grad():\n",
    "                for group in shape_groups:\n",
    "                    if (\n",
    "                        hasattr(group, \"fill_color\")\n",
    "                        and group.fill_color is not None\n",
    "                        and group.fill_color.grad is not None\n",
    "                    ):\n",
    "                        group.fill_color.grad[3] = 0.0  # Ignore alpha channel in gradient\n",
    "                    if (\n",
    "                        hasattr(group, \"stroke_color\")\n",
    "                        and group.stroke_color is not None\n",
    "                        and group.stroke_color.grad is not None\n",
    "                    ):\n",
    "                        group.stroke_color.grad[3] = 0.0  # Ignore alpha channel in gradient\n",
    "        except Exception as e:\n",
    "            print(f\"  Error: Exception occurred during gradient computation (iter {t}): {e}\")\n",
    "            break\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            [p for pg in param_groups for p in pg[\"params\"] if p.grad is not None],\n",
    "            max_norm=args.grad_clip_norm\n",
    "        )\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clamp values to valid ranges after update\n",
    "        with torch.no_grad():\n",
    "            for group in shape_groups:\n",
    "                if hasattr(group, \"fill_color\") and group.fill_color is not None and group.fill_color.requires_grad:\n",
    "                    group.fill_color.data.clamp_(0.0, 1.0)\n",
    "                if (\n",
    "                    hasattr(group, \"stroke_color\")\n",
    "                    and group.stroke_color is not None\n",
    "                    and group.stroke_color.requires_grad\n",
    "                ):\n",
    "                    group.stroke_color.data.clamp_(0.0, 1.0)\n",
    "            for shape in shapes:\n",
    "                if (\n",
    "                    hasattr(shape, \"stroke_width\")\n",
    "                    and shape.stroke_width is not None\n",
    "                    and shape.stroke_width.requires_grad\n",
    "                ):\n",
    "                    shape.stroke_width.data.relu_()\n",
    "                if hasattr(shape, \"radius\") and shape.radius is not None and shape.radius.requires_grad:\n",
    "                    shape.radius.data.clamp_(min=1.0)\n",
    "\n",
    "        current_loss = loss.item()\n",
    "\n",
    "        # If after aesthetic_iter and loss is improved, update best result\n",
    "        if t >= args.aesthetic_iter and current_loss < best_loss:\n",
    "            best_loss = current_loss\n",
    "            best_iteration = t\n",
    "\n",
    "        # Logging\n",
    "        if (t + 1) % args.log_interval == 0 or t == 0:\n",
    "            with torch.no_grad():\n",
    "                current_aesthetic_score_torch_avg = aesthetic_score_batch.mean().item()\n",
    "                current_similarity_loss_raw_avg = similarity_loss_raw.item()  # PaliGemma loss is already scalar\n",
    "                current_similarity_loss_avg = similarity_loss.item()\n",
    "\n",
    "            elapsed_time = time.time() - start_time\n",
    "            lrs = {pg[\"name\"]: pg[\"lr\"] for pg in optimizer.param_groups}\n",
    "            lr_str = \"/\".join([f\"{lr:.6f}\" for lr in lrs.values()])\n",
    "            print(\n",
    "                f\"    Iter [{(t + 1):>4}/{args.iterations}], \"\n",
    "                f\"LRs: {lr_str}, \"\n",
    "                f\"AesScore(T Avg): {current_aesthetic_score_torch_avg:.6f}, \"\n",
    "                f\"{similarity_mode.upper()}Loss(Raw Avg): {current_similarity_loss_raw_avg:.6f}, \"\n",
    "                f\"{similarity_mode.upper()}Loss(Norm/Direct Avg): {current_similarity_loss_avg:.6f}, \"\n",
    "                f\"Loss(A/Sim/Mse/T Avg): {aesthetic_loss.item():.4f}/{similarity_loss.item():.4f}/{mse_loss.item():.4f}/{loss.item():.6f}, \"\n",
    "                f\"Best Loss: {best_loss:.6f} (iter {best_iteration}), \"\n",
    "                f\"Time: {elapsed_time:.2f}s\"\n",
    "            )\n",
    "\n",
    "    print(f\"  Optimization completed! Best Loss: {best_loss:.6f} at iteration {best_iteration}\")\n",
    "    return shapes, shape_groups, -aesthetic_loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23e65340",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import re\n",
    "import tempfile\n",
    "import os\n",
    "import vtracer\n",
    "\n",
    "def svg_conversion(img, image_size=(256,256)):\n",
    "    tmp_dir = tempfile.TemporaryDirectory()\n",
    "    # Open the image, resize it, and save it to the temporary directory\n",
    "    resized_img = img.resize(image_size)\n",
    "    tmp_file_path = os.path.join(tmp_dir.name, \"tmp.png\")\n",
    "    resized_img = resized_img.convert(\"RGB\")\n",
    "    resized_img.save(tmp_file_path)\n",
    "    \n",
    "    svg_path = os.path.join(tmp_dir.name, \"gen_svg.svg\")\n",
    "    vtracer.convert_image_to_svg_py(\n",
    "                tmp_file_path,\n",
    "                svg_path,\n",
    "                colormode=\"color\",  # [\"color\"] or \"binary\"\n",
    "                # hierarchical=\"cutout\",  # [\"stacked\"] or \"cutout\"\n",
    "                hierarchical=\"stacked\",\n",
    "                mode=\"polygon\",\n",
    "            )\n",
    "    \n",
    "    with open(svg_path, 'r', encoding='utf-8') as f:\n",
    "        svg_str = f.read()\n",
    "    \n",
    "    return svg_str\n",
    "\n",
    "def svg_conversion_division(img: Image.Image, image_size=(384, 384), num_divisions=5):\n",
    "    \"\"\"\n",
    "    1. Resize the image based on image_size and divide it into num_divisions x num_divisions tiles.\n",
    "    2. For each tile, crop the original image accordingly and convert it into SVG.\n",
    "    3. Finally, merge all the tile SVG paths, adjusting coordinates directly instead of using transforms.\n",
    "    \"\"\"\n",
    "    # Resize the input image to the specified image_size\n",
    "    img_resized = img.resize(image_size, Image.LANCZOS)\n",
    "    W, H = img_resized.size\n",
    "\n",
    "    # Calculate the base size for each tile\n",
    "    base_tile_w = W // num_divisions\n",
    "    base_tile_h = H // num_divisions\n",
    "\n",
    "    if base_tile_w == 0 or base_tile_h == 0:\n",
    "        # If the resulting tile size becomes 0, issue a warning and continue with minimal size\n",
    "        print(\n",
    "            f\"Warning: Calculated base tile size is very small ({base_tile_w}x{base_tile_h}). \"\n",
    "            \"Resulting SVG might be distorted or empty.\"\n",
    "        )\n",
    "        # Set minimum size to 1 to continue processing\n",
    "        base_tile_w = max(1, base_tile_w)\n",
    "        base_tile_h = max(1, base_tile_h)\n",
    "\n",
    "    all_shifted_path_tags = []\n",
    "    overall_bg_color = None  # Background color of the entire SVG\n",
    "\n",
    "    for r_idx in range(num_divisions):\n",
    "        for c_idx in range(num_divisions):\n",
    "            offset_x = c_idx * base_tile_w\n",
    "            offset_y = r_idx * base_tile_h\n",
    "\n",
    "            # Calculate the actual width and height for the current tile (last tiles may include remainders)\n",
    "            current_tile_w = base_tile_w if c_idx < num_divisions - 1 else W - offset_x\n",
    "            current_tile_h = base_tile_h if r_idx < num_divisions - 1 else H - offset_y\n",
    "\n",
    "            if current_tile_w <= 0 or current_tile_h <= 0:\n",
    "                continue  # Skip tiles with zero or negative size\n",
    "\n",
    "            # Crop the tile from the resized image\n",
    "            box = (offset_x, offset_y, offset_x + current_tile_w, offset_y + current_tile_h)\n",
    "            tile_img = img_resized.crop(box)\n",
    "\n",
    "            if tile_img.width == 0 or tile_img.height == 0:\n",
    "                continue\n",
    "\n",
    "            # Same logic as svg_conversion: convert tile to SVG using vtracer\n",
    "            try:\n",
    "                with tempfile.TemporaryDirectory() as tmp_dir_name:\n",
    "                    tmp_file_path = os.path.join(tmp_dir_name, f\"tmp_tile_{r_idx}_{c_idx}.png\")\n",
    "                    # Save in RGB mode\n",
    "                    tile_img.convert(\"RGB\").save(tmp_file_path)\n",
    "\n",
    "                    svg_tile_output_path = os.path.join(tmp_dir_name, f\"tile_gen_svg_{r_idx}_{c_idx}.svg\")\n",
    "                    vtracer.convert_image_to_svg_py(\n",
    "                        tmp_file_path,\n",
    "                        svg_tile_output_path,\n",
    "                        colormode=\"color\",\n",
    "                        # hierarchical=\"cutout\",  # Use cutout mode\n",
    "                        hierarchical=\"stacked\",\n",
    "                        mode=\"polygon\",\n",
    "                        # Add other vtracer parameters here if needed\n",
    "                    )\n",
    "                    with open(svg_tile_output_path, encoding=\"utf-8\") as f:\n",
    "                        tile_svg_str = f.read()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing tile ({r_idx},{c_idx}): {e}\")\n",
    "                continue\n",
    "\n",
    "            # Extract <path> elements from the tile SVG\n",
    "            raw_path_tags_from_tile = extract_paths(tile_svg_str)\n",
    "\n",
    "            if not raw_path_tags_from_tile:\n",
    "                continue\n",
    "\n",
    "            # Try to get the background color from the first path of the first tile\n",
    "            if r_idx == 0 and c_idx == 0 and not overall_bg_color:\n",
    "                first_path_of_first_tile = raw_path_tags_from_tile[0]\n",
    "                match_bg = re.search(r'fill=\"([^\"]+)\"', first_path_of_first_tile)\n",
    "                if match_bg:\n",
    "                    overall_bg_color = match_bg.group(1)\n",
    "\n",
    "            # Offset the coordinates of each path\n",
    "            for path_idx, path_tag_original in enumerate(raw_path_tags_from_tile):\n",
    "                d_match = re.search(r'd=\"([^\"]+)\"', path_tag_original)\n",
    "                if not d_match:\n",
    "                    continue\n",
    "                d_original = d_match.group(1)\n",
    "\n",
    "                shifted_sub_paths_strings = []\n",
    "                # Split into subpaths using \"M...Z\" pattern\n",
    "                sub_paths_data = re.findall(r\"M[^M]*?Z\", d_original, flags=re.IGNORECASE)\n",
    "                if not sub_paths_data and d_original.strip():  # If no subpaths, treat as one whole path\n",
    "                    sub_paths_data = [d_original]\n",
    "\n",
    "                valid_path_data_found = False\n",
    "                for sub_d_segment in sub_paths_data:\n",
    "                    # Tokens are commands (letters) or numbers\n",
    "                    tokens = re.findall(r\"[MLZHVCSQTA]|-?[\\d\\.]+\", sub_d_segment, flags=re.IGNORECASE)\n",
    "                    shifted_tokens_for_sub = []\n",
    "                    k = 0\n",
    "                    while k < len(tokens):\n",
    "                        cmd = tokens[k]\n",
    "                        shifted_tokens_for_sub.append(cmd)\n",
    "                        k += 1\n",
    "\n",
    "                        # Handle coordinates based on the number of arguments for each command\n",
    "                        # vtracer polygon mode is expected to use mainly M and L (absolute coordinates)\n",
    "                        # Assume coordinates are integers based on vtracer polygon output\n",
    "                        coords_to_process = 0\n",
    "                        if cmd.upper() in (\"M\", \"L\", \"T\"):\n",
    "                            coords_to_process = 1  # One x,y pair\n",
    "                        elif cmd.upper() in (\"Q\", \"S\"):\n",
    "                            coords_to_process = 2  # Two x,y pairs\n",
    "                        elif cmd.upper() == \"C\":\n",
    "                            coords_to_process = 3  # Three x,y pairs\n",
    "                        # A (Arc) is complex and assumed to be unused in polygon mode\n",
    "                        # H, V are single coordinates\n",
    "\n",
    "                        if cmd.upper() == \"H\":  # Horizontal line\n",
    "                            if k < len(tokens):\n",
    "                                try:\n",
    "                                    x = int(float(tokens[k]))\n",
    "                                    shifted_tokens_for_sub.append(str(x + offset_x))\n",
    "                                    k += 1\n",
    "                                except (ValueError, IndexError):\n",
    "                                    break  # Parse error\n",
    "                        elif cmd.upper() == \"V\":  # Vertical line\n",
    "                            if k < len(tokens):\n",
    "                                try:\n",
    "                                    y = int(float(tokens[k]))\n",
    "                                    shifted_tokens_for_sub.append(str(y + offset_y))\n",
    "                                    k += 1\n",
    "                                except (ValueError, IndexError):\n",
    "                                    break  # Parse error\n",
    "                        else:  # Commands with x,y pairs\n",
    "                            for _ in range(coords_to_process):\n",
    "                                if k + 1 < len(tokens):\n",
    "                                    try:\n",
    "                                        x = int(float(tokens[k]))\n",
    "                                        y = int(float(tokens[k + 1]))\n",
    "                                        shifted_tokens_for_sub.append(str(x + offset_x))\n",
    "                                        shifted_tokens_for_sub.append(str(y + offset_y))\n",
    "                                        k += 2\n",
    "                                    except (ValueError, IndexError):\n",
    "                                        k = len(tokens)  # Exit loop on error\n",
    "                                        break\n",
    "                                else:  # Not enough tokens\n",
    "                                    k = len(tokens)\n",
    "                                    break\n",
    "                            if k == len(tokens) and _ < coords_to_process - 1:  # Ended prematurely\n",
    "                                shifted_tokens_for_sub = [shifted_tokens_for_sub[0]]  # Only keep the command\n",
    "\n",
    "                    if len(shifted_tokens_for_sub) > 1:  # Command + at least one argument\n",
    "                        shifted_sub_paths_strings.append(\" \".join(shifted_tokens_for_sub))\n",
    "                        valid_path_data_found = True\n",
    "\n",
    "                if valid_path_data_found:\n",
    "                    shifted_d_final = \"\".join(shifted_sub_paths_strings)\n",
    "                    # Replace only the d attribute in the original path tag\n",
    "                    new_path_tag = re.sub(r'd=\"[^\"]*\"', f'd=\"{shifted_d_final}\"', path_tag_original, count=1)\n",
    "                    all_shifted_path_tags.append(new_path_tag)\n",
    "\n",
    "    # Construct overall SVG header using resized width and height\n",
    "    svg_final_header = f'<svg width=\"{W}\" height=\"{H}\" viewBox=\"0 0 {W} {H}\" xmlns=\"http://www.w3.org/2000/svg\">'\n",
    "\n",
    "    # Concatenate all adjusted paths\n",
    "    final_svg_str = svg_final_header + \"\".join(all_shifted_path_tags) + \"</svg>\"\n",
    "\n",
    "    return final_svg_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fef6a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_transform_from_path(path_text: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes an SVG <path> tag string and:\n",
    "    - Removes transform=\"translate(tx,ty)\"\n",
    "    - Adds (tx, ty) to all coordinates in the `d` attribute\n",
    "    Then returns the modified string.\n",
    "    \"\"\"\n",
    "    # 1) Extract tx, ty from the transform attribute\n",
    "    m_tx = re.search(\n",
    "        r'transform=([\"\\'])\\s*translate\\(\\s*([-+]?\\d*\\.?\\d+)\\s*,\\s*([-+]?\\d*\\.?\\d+)\\s*\\)\\s*\\1',\n",
    "        path_text\n",
    "    )\n",
    "    if not m_tx:\n",
    "        # If there's no transform, return as is\n",
    "        return path_text\n",
    "\n",
    "    tx, ty = float(m_tx.group(2)), float(m_tx.group(3))\n",
    "\n",
    "    # 2) Extract the content of the `d` attribute\n",
    "    m_d = re.search(r'd=([\"\\'])(?P<d>.*?)\\1', path_text)\n",
    "    if not m_d:\n",
    "        # If `d` attribute not found, just remove transform and return\n",
    "        return re.sub(r'\\s+transform=([\"\\']).*?\\1', '', path_text)\n",
    "\n",
    "    d_orig = m_d.group('d')\n",
    "\n",
    "    # 3) Find coordinate pairs “x,y” and shift them\n",
    "    def shift_coord(m):\n",
    "        x, y = float(m.group(1)), float(m.group(2))\n",
    "        x2, y2 = x + tx, y + ty\n",
    "\n",
    "        def fmt(v: float) -> str:\n",
    "            return str(int(v)) if v.is_integer() else ('%f' % v).rstrip('0').rstrip('.')\n",
    "\n",
    "        return fmt(x2) + ',' + fmt(y2)\n",
    "\n",
    "    d_shifted = re.sub(r'([-+]?\\d*\\.?\\d+),\\s*([-+]?\\d*\\.?\\d+)', shift_coord, d_orig)\n",
    "\n",
    "    # 4) Replace the old `d` attribute with the new shifted one, and remove transform\n",
    "    start, end = m_d.span()\n",
    "    before_d = path_text[:start]\n",
    "    after_d  = path_text[end:]\n",
    "    new_path = before_d + f'd=\"{d_shifted}\"' + after_d\n",
    "    new_path = re.sub(r'\\s+transform=([\"\\']).*?\\1', '', new_path)\n",
    "\n",
    "    return new_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37fc09c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paths(svg):\n",
    "    \"\"\"\n",
    "    Extracts all <path> opening tags from an SVG string,\n",
    "    removes transform attributes, and returns a list\n",
    "    of cleaned path tags in their original order.\n",
    "    \"\"\"\n",
    "    # ① Extract only <path> tags in order\n",
    "    tag_pattern = re.compile(r'<path\\b[^>]*>', flags=re.IGNORECASE)\n",
    "    raw_tags = [m.group(0) for m in tag_pattern.finditer(svg)]\n",
    "    \n",
    "    # ② Clean each path tag string\n",
    "    return [remove_transform_from_path(t) for t in raw_tags]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26179115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_svg_path_vt(elem: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a <path> element in M x,y L ... Z format,\n",
    "    and returns the shortest equivalent path code that\n",
    "    draws the same shape.\n",
    "    Optimizes all \"M…Z\" subpaths in order.\n",
    "    \"\"\"\n",
    "    # --- Extract attributes --------------------------------------------------\n",
    "    d_match = re.search(r'd=\"([^\"]+)\"', elem)\n",
    "    if not d_match:\n",
    "        return elem\n",
    "    d_raw = d_match.group(1)\n",
    "\n",
    "    fill_m = re.search(r'fill=\"([^\"]+)\"', elem)\n",
    "    fill = fill_m.group(1) if fill_m else None\n",
    "\n",
    "    # --- Split by subpaths (M…Z blocks) -------------------------------------\n",
    "    subpaths = re.findall(r'M[^M]*?Z', d_raw)\n",
    "\n",
    "    optimized_subs = []\n",
    "    for sub in subpaths:\n",
    "        # --- Convert string to coordinate list -------------------------------\n",
    "        tokens = re.findall(r'[MLZ]|-?\\d+', sub)\n",
    "        pts, cmd, i = [], None, 0\n",
    "        while i < len(tokens):\n",
    "            t = tokens[i]\n",
    "            if t in ('M', 'L', 'Z'):\n",
    "                cmd, i = t, i + 1\n",
    "                continue\n",
    "            if cmd in ('M', 'L'):\n",
    "                pts.append((int(t), int(tokens[i + 1])))\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        # Skip this subpath if no valid coordinates\n",
    "        if not pts:\n",
    "            continue\n",
    "\n",
    "        # --- Build the shortest path command sequence ------------------------\n",
    "        d_parts = [f'M{pts[0][0]} {pts[0][1]}']\n",
    "        prev_x, prev_y = pts[0]\n",
    "\n",
    "        for x, y in pts[1:]:\n",
    "            dx, dy = x - prev_x, y - prev_y\n",
    "\n",
    "            cands = []\n",
    "            # ― Absolute commands ―\n",
    "            if dy == 0:\n",
    "                cands.append(f'H{x}')\n",
    "            if dx == 0:\n",
    "                cands.append(f'V{y}')\n",
    "            cands.append(f'L{x} {y}')\n",
    "            # ― Relative commands ―\n",
    "            if dy == 0:\n",
    "                cands.append(f'h{dx}')\n",
    "            if dx == 0:\n",
    "                cands.append(f'v{dy}')\n",
    "            cands.append(f'l{dx} {dy}')\n",
    "\n",
    "            # Choose the shortest command\n",
    "            d_parts.append(min(cands, key=len))\n",
    "            prev_x, prev_y = x, y\n",
    "\n",
    "        d_parts.append('Z')\n",
    "        # Skip if only \"M\" and \"Z\"\n",
    "        if len(d_parts) > 2:\n",
    "            optimized_subs.append(''.join(d_parts))\n",
    "\n",
    "    if not optimized_subs:\n",
    "        return None\n",
    "\n",
    "    d_optimized = ''.join(optimized_subs)\n",
    "    return f'<path d=\"{d_optimized}\"' + (f' fill=\"{fill}\"' if fill else '') + '/>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bad2754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_svg_size_with_regex(svg):\n",
    "    \"\"\"Extract width and height from an SVG tag using regular expressions\"\"\"\n",
    "    w_match = re.search(r'<svg[^>]*\\bwidth=[\"\\']([^\"\\']+)[\"\\']', svg)\n",
    "    h_match = re.search(r'<svg[^>]*\\bheight=[\"\\']([^\"\\']+)[\"\\']', svg)\n",
    "    width  = w_match.group(1) if w_match else None\n",
    "    height = h_match.group(1) if h_match else None\n",
    "    return width, height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6fd98b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svg_compress(svg):\n",
    "    \"\"\"\n",
    "    Compresses an SVG string by:\n",
    "    - Extracting all paths\n",
    "    - Optimizing each path to its shortest form\n",
    "    - Rebuilding a compact SVG with consistent header and footer\n",
    "    \"\"\"\n",
    "    path_list = extract_paths(svg)\n",
    "    path_list = [optimize_svg_path_vt(p) for p in path_list]\n",
    "    path_list = [p for p in path_list if p is not None]\n",
    "\n",
    "    width, height = extract_svg_size_with_regex(svg)\n",
    "    header = f'<svg width=\"384\" height=\"384\" viewBox=\"0 0 {width} {height}\">'\n",
    "    fill_background = re.search(r'fill=\"([^\"]+)\"', path_list[0])\n",
    "    header += f'<rect width=\"{width}\" height=\"{height}\" fill=\"{fill_background.group(1)}\"/>'\n",
    "    \n",
    "    # footer = f'<path d=\"M167 200h20l-20 30h20\" fill=\"none\" stroke=\"white\" stroke-width=\"{4*int(width)/256}\" transform=\"scale({int(width)/256} {int(height)/256})\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/><path d=\"M167 200h20l-20 30h20\" fill=\"none\" stroke=\"#EDCE6A\" stroke-width=\"{2*int(width)/256}\" transform=\"scale({int(width)/256} {int(height)/256})\" stroke-linecap=\"round\" stroke-linejoin=\"round\"/></svg>'\n",
    "    footer = \"</svg>\"\n",
    "    return header + ''.join(path_list[1:]) + footer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04a7b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtracer_png_to_svg(image, size_range=(50, 500), limit=9500):\n",
    "    \"\"\"\n",
    "    Convert the given image to an SVG and compress it,\n",
    "    using binary search to find the largest size such that\n",
    "    the length of the resulting SVG string does not exceed `limit`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    image : any\n",
    "        Input image object to be vectorized into SVG.\n",
    "    size_range : tuple(int, int)\n",
    "        Minimum and maximum image sizes (inclusive) to search over.\n",
    "    limit : int\n",
    "        Maximum allowed length (in characters) for the SVG string.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_size : int\n",
    "        The largest image size for which len(svg) <= limit.\n",
    "    best_svg : str\n",
    "        The SVG string converted and compressed at best_size.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If no image size within `size_range` can satisfy the length limit.\n",
    "    \"\"\"\n",
    "    lo, hi = size_range\n",
    "    best_size = None\n",
    "    best_svg = None\n",
    "\n",
    "    # Binary search to find the largest size that keeps SVG under limit\n",
    "    while abs(lo - hi) > 5:\n",
    "        mid = (lo + hi) // 2\n",
    "\n",
    "        # Convert to SVG and compress\n",
    "        svg = svg_conversion(image, (mid, mid))\n",
    "        svg = svg_compress(svg)\n",
    "        length = len(svg)\n",
    "\n",
    "        if length <= limit:\n",
    "            # This size is valid → Try larger sizes\n",
    "            best_size = mid\n",
    "            best_svg = svg\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            # SVG too large → Try smaller sizes\n",
    "            hi = mid - 1\n",
    "\n",
    "    # If size is small, try subdividing the image and recompress\n",
    "    if best_size < 256:\n",
    "        return best_svg\n",
    "    else:\n",
    "        for i in range(2, 14):\n",
    "            svg = svg_conversion_division(image, (256, 256), i)\n",
    "            svg = svg_compress(svg)\n",
    "            length = len(svg)\n",
    "            if length < 9000:\n",
    "                best_svg = svg\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    return best_svg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d85496e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import time  # Added for plotting and measuring execution time\n",
    "import contextlib\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F  # Added for interpolate\n",
    "import torchvision.transforms as transforms\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def svg_to_png(svg_code: str, size: tuple = (384, 384)) -> Image.Image:\n",
    "    \"\"\"\n",
    "    Converts an SVG string to a PNG image using CairoSVG.\n",
    "    If the SVG does not define a `viewBox`, it will add one using the provided size.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    svg_code : str\n",
    "        The SVG string to convert.\n",
    "    size : tuple[int, int], default=(384, 384)\n",
    "        The desired size of the output PNG image (width, height).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    PIL.Image.Image\n",
    "        The generated PNG image.\n",
    "    \"\"\"\n",
    "    # Add viewBox if not present\n",
    "    if 'viewBox' not in svg_code:\n",
    "        svg_code = svg_code.replace('<svg', f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
    "\n",
    "    # Convert SVG to PNG\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode('utf-8'))\n",
    "    return Image.open(io.BytesIO(png_data)).convert('RGB').resize(size)\n",
    "\n",
    "\n",
    "def tensor_to_pil(tensor):\n",
    "    \"\"\"\n",
    "    Convert a tensor in HWC format to a PIL Image.\n",
    "    \"\"\"\n",
    "    if tensor.dim() == 4:\n",
    "        tensor = tensor.squeeze(0)  # Remove batch dimension\n",
    "\n",
    "    # Ensure RGB format\n",
    "    if tensor.shape[2] == 4:  # RGBA format\n",
    "        tensor = tensor[:, :, :3]  # Remove alpha channel\n",
    "\n",
    "    # Convert from [0, 1] to [0, 255]\n",
    "    tensor = tensor.cpu().detach() * 255\n",
    "    tensor = tensor.to(torch.uint8)\n",
    "\n",
    "    # Convert to PIL image (expects HWC format)\n",
    "    img = Image.fromarray(tensor.numpy())\n",
    "    return img\n",
    "\n",
    "\n",
    "def extract_rect_and_path(svg_text, resize):\n",
    "    \"\"\"\n",
    "    Extract opening <rect> and <path> tags from SVG string,\n",
    "    removing opacity, converting RGB to hex, and rounding values.\n",
    "\n",
    "    Returns:\n",
    "        A list of processed tag strings in order of appearance.\n",
    "    \"\"\"\n",
    "    tag_pattern = re.compile(r'<(?:rect|path)\\b[^>]*>', flags=re.IGNORECASE)\n",
    "    raw_tags = [m.group(0) for m in tag_pattern.finditer(svg_text)]\n",
    "\n",
    "    return [ _process_tag(t, resize) for t in raw_tags ]\n",
    "\n",
    "\n",
    "def _process_tag(tag, resize):\n",
    "    # 1) Remove `opacity` attributes\n",
    "    tag = re.sub(r'\\s*opacity\\s*=\\s*\"[^\"]*\"', '', tag, flags=re.IGNORECASE)\n",
    "\n",
    "    # 2) Convert fill=\"rgb(R,G,B)\" to fill=\"#RRGGBB\"\n",
    "    def _rgb_to_hex(m: re.Match) -> str:\n",
    "        nums = [float(m.group(i)) for i in (1,2,3)]\n",
    "        ints = [int(round(v)) for v in nums]\n",
    "        return f'fill=\"#{ints[0]:02X}{ints[1]:02X}{ints[2]:02X}\"'\n",
    "    \n",
    "    tag = re.sub(\n",
    "        r'fill\\s*=\\s*\"rgb\\(\\s*([0-9]+(?:\\.[0-9]+)?)\\s*,\\s*([0-9]+(?:\\.[0-9]+)?)\\s*,\\s*([0-9]+(?:\\.[0-9]+)?)\\s*\\)\"',\n",
    "        _rgb_to_hex,\n",
    "        tag,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # 3) Round all decimal numbers and scale by resize factor\n",
    "    def _round_num(m: re.Match) -> str:\n",
    "        return str(int(round(float(m.group(0)) * resize / 384)))\n",
    "    \n",
    "    tag = re.sub(r'-?\\d+\\.\\d+', _round_num, tag)\n",
    "    return tag\n",
    "\n",
    "\n",
    "def _close_path_with_Z(d: str) -> str:\n",
    "    \"\"\"\n",
    "    If the path string ends with 'L x y', replace it with 'Z'.\n",
    "    Otherwise, return the original path.\n",
    "    \"\"\"\n",
    "    m = re.match(r'^(.*?)(?:\\s+L\\s+[-+]?\\d*\\.?\\d+(?:[,\\s]+[-+]?\\d*\\.?\\d+)\\s*)$', d)\n",
    "    if m:\n",
    "        return m.group(1) + ' Z'\n",
    "    return d\n",
    "\n",
    "\n",
    "def merge_paths_by_fill(paths: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Merge <path> elements with the same fill color.\n",
    "    - Concatenate `d` attributes.\n",
    "    - Close each subpath with 'Z' if necessary.\n",
    "    \n",
    "    Returns:\n",
    "        List of <path> elements with merged `d` and same fill.\n",
    "    \"\"\"\n",
    "    paths_by_fill = defaultdict(list)\n",
    "\n",
    "    for path_str in paths:\n",
    "        # Extract `d` attribute\n",
    "        d_match = re.search(r'd\\s*=\\s*[\"\\']([^\"\\']+)[\"\\']', path_str)\n",
    "        if not d_match:\n",
    "            continue\n",
    "        d = d_match.group(1)\n",
    "\n",
    "        # Extract fill attribute\n",
    "        fill_match = re.search(r'fill\\s*=\\s*[\"\\']([^\"\\']*)[\"\\']', path_str)\n",
    "        fill = fill_match.group(1) if fill_match else ''\n",
    "\n",
    "        # Fix closing\n",
    "        d_fixed = _close_path_with_Z(d)\n",
    "        paths_by_fill[fill].append(d_fixed)\n",
    "\n",
    "    # Merge each group\n",
    "    merged_paths = []\n",
    "    for fill, d_list in paths_by_fill.items():\n",
    "        merged_d = \" \".join(d_list)\n",
    "        fill_attr = f' fill=\"{fill}\"' if fill else ''\n",
    "        merged_path = f'<path d=\"{merged_d}\"{fill_attr} />'\n",
    "        merged_paths.append(merged_path)\n",
    "\n",
    "    return merged_paths\n",
    "\n",
    "\n",
    "def _shortest_segment(prev, cur):\n",
    "    \"\"\"\n",
    "    Return the shortest SVG command for moving from `prev` to `cur`.\n",
    "    Uses relative and absolute commands and chooses the shortest.\n",
    "    \"\"\"\n",
    "    px, py = prev\n",
    "    x, y = cur\n",
    "    dx, dy = x - px, y - py\n",
    "    cands = [\n",
    "        (f'H{x}'      , abs(dy)==0),           # Absolute horizontal\n",
    "        (f'h{dx}'     , abs(dy)==0),           # Relative horizontal\n",
    "        (f'V{y}'      , abs(dx)==0),           # Absolute vertical\n",
    "        (f'v{dy}'     , abs(dx)==0),           # Relative vertical\n",
    "        (f'L{x} {y}'  , True),                 # Absolute diagonal\n",
    "        (f'l{dx} {dy}', True),                 # Relative diagonal\n",
    "    ]\n",
    "    # Filter by condition and select the shortest string\n",
    "    return min((s for s, ok in cands if ok), key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da3234ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_path(pts):\n",
    "    \"\"\"List of points → shortest SVG command sequence (with Z)\"\"\"\n",
    "    d = [f'M{pts[0][0]} {pts[0][1]}']\n",
    "    for a, b in zip(pts, pts[1:]):\n",
    "        d.append(_shortest_segment(a, b))\n",
    "    d.append('')\n",
    "    return ''.join(d)\n",
    "\n",
    "def svg_path_extream(elem: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Given a <path> element (possibly multiple M…Z subpaths),\n",
    "    - Try all starting points (cyclic permutation)\n",
    "    - Try both clockwise and counter-clockwise\n",
    "    and return the shortest possible path `d`.\n",
    "\n",
    "    Returns None if no valid path is found.\n",
    "    \"\"\"\n",
    "    # --- Extract attributes -------------------------------------------------\n",
    "    m_d = re.search(r'd=\"([^\"]+)\"', elem)\n",
    "    d_raw = m_d.group(1)\n",
    "\n",
    "    fill_m = re.search(r'fill=\"([^\"]+)\"', elem)\n",
    "    fill   = f' fill=\"{fill_m.group(1)}\"'\n",
    "\n",
    "    # --- Extract subpaths (M…Z) ---------------------------------------------\n",
    "    subs_raw = re.findall(r'M[^M]*?Z', d_raw)\n",
    "    optimized = []\n",
    "\n",
    "    for sub in subs_raw:\n",
    "        # Tokenize to points\n",
    "        toks = sub.replace(',', ' ').split()\n",
    "        pts, cmd, i = [], None, 0\n",
    "        while i < len(toks):\n",
    "            t = toks[i]\n",
    "            if t in ('M', 'L', 'Z'):\n",
    "                cmd, i = t, i + 1\n",
    "            elif cmd in ('M', 'L'):\n",
    "                pt = (int(float(t)), int(float(toks[i + 1])))\n",
    "                if len(pts) == 0 or pts[-1] != pt:\n",
    "                    pts.append(pt)\n",
    "                i += 2\n",
    "            else:\n",
    "                i += 1\n",
    "        if len(pts) < 3:  # Skip if not enough points for a shape\n",
    "            continue\n",
    "\n",
    "        # --- Try all N×2 permutations (start point × direction) --------------\n",
    "        best, best_len = sub, len(sub)\n",
    "        for seq in (pts, pts[::-1]):  # Normal and reversed\n",
    "            n = len(seq)\n",
    "            for k in range(n):\n",
    "                rot = seq[k:] + seq[:k]  # Rotate starting point\n",
    "                d_candidate = _encode_path(rot)\n",
    "                if (l := len(d_candidate)) < best_len:\n",
    "                    best, best_len = d_candidate, l\n",
    "\n",
    "        optimized.append(best)\n",
    "\n",
    "    if not optimized:  # No valid shapes\n",
    "        return None\n",
    "\n",
    "    d_final = ''.join(optimized)\n",
    "    return f'<path d=\"{d_final}\"{fill}/>'\n",
    "\n",
    "\n",
    "def optimize_svg_size(svg, resize=384, limit=False):\n",
    "    \"\"\"\n",
    "    Resize and optimize an SVG for a fixed canvas.\n",
    "    If `limit` is True, it trims the <path> elements to fit within 10KB.\n",
    "\n",
    "    Returns an optimized SVG string.\n",
    "    \"\"\"\n",
    "    canvas_width = resize\n",
    "    canvas_height = resize\n",
    "    header = f'<svg width=\"384\" height=\"384\" viewBox=\"0 0 {canvas_width} {canvas_height}\">'\n",
    "    # footer = f'<g fill=\"none\" transform=\"scale({round(canvas_width/256,2)} {round(canvas_height/256,2)})\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M167 200h20l-20 30h20\" stroke=\"#fff\" stroke-width=\"4\"/><path d=\"M167 200h20l-20 30h20\" stroke=\"#EDCE6A\" stroke-width=\"2\"/></g></svg>'\n",
    "    footer = \"</svg>\"\n",
    "    # Ignore text layers (last 2 elements)\n",
    "    svg_opt_list = extract_rect_and_path(svg, resize)[:-2]\n",
    "\n",
    "    # Split background <rect> and <path> elements\n",
    "    rect = svg_opt_list[0]\n",
    "    rect = re.sub(r'\\s*(?:x|y)=\"0\"', '', rect)\n",
    "    header += rect\n",
    "\n",
    "    path_list = svg_opt_list[1:]\n",
    "    # Merge paths with same fill color\n",
    "    path_list = merge_paths_by_fill(path_list)\n",
    "\n",
    "    # Compress and optimize paths\n",
    "    for i in range(len(path_list)):\n",
    "        path_list[i] = svg_path_extream(path_list[i])\n",
    "\n",
    "    # Remove any None results (invalid paths)\n",
    "    path_list = [p for p in path_list if p is not None]\n",
    "\n",
    "    svg_opt = header + ''.join(path_list) + footer\n",
    "\n",
    "    # Trim to 10KB if limit is True\n",
    "    if limit:\n",
    "        len_tmp = len((header + footer).encode())\n",
    "        idx = 0\n",
    "        for i in range(len(path_list)):\n",
    "            len_tmp += len(path_list[i].encode())\n",
    "            if len_tmp > 10000:\n",
    "                break\n",
    "            idx += 1\n",
    "        svg_opt = header + \"\".join(path_list[:idx]) + footer\n",
    "\n",
    "    return svg_opt\n",
    "\n",
    "\n",
    "def optimize_svg_10k(svg):\n",
    "    \"\"\"\n",
    "    Resize and optimize an SVG to fit under 10,000 bytes.\n",
    "    Uses binary search to find the maximum allowed size.\n",
    "\n",
    "    If it fails, forcibly trims to fit using `limit=True`.\n",
    "\n",
    "    Returns an optimized SVG string.\n",
    "    \"\"\"\n",
    "    lo, hi = (150, 500)\n",
    "    best_size = None\n",
    "    best_svg = None\n",
    "\n",
    "    while abs(lo - hi) > 3:\n",
    "        mid = (lo + hi) // 2\n",
    "\n",
    "        # Resize and optimize\n",
    "        svg_opt = optimize_svg_size(svg, mid)\n",
    "        length = len(svg_opt.encode())\n",
    "\n",
    "        if length <= 10000:\n",
    "            # Acceptable → try larger size\n",
    "            best_size = mid\n",
    "            best_svg = svg_opt\n",
    "            lo = mid + 1\n",
    "        else:\n",
    "            # Too large → reduce size\n",
    "            hi = mid - 1\n",
    "    \n",
    "    if best_size is None:\n",
    "        print(\"Failed to reduce under 10,000 bytes. Removing some paths.\")\n",
    "        svg_opt = optimize_svg_size(svg, 150, limit=True)\n",
    "        return svg_opt\n",
    "    else:\n",
    "        print(f\"Resized to {best_size}\")\n",
    "\n",
    "    return best_svg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07d9df56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizationArgs:\n",
    "    iterations = 200\n",
    "    jpeg_iter = 200\n",
    "    aesthetic_iter = 0\n",
    "    warmup_iter = 0\n",
    "    log_interval = 10\n",
    "    w_aesthetic = 100.0\n",
    "    w_siglip = 100.0\n",
    "    w_mse = 5000.0\n",
    "    batch_size = 1\n",
    "    lr_points = 0.3\n",
    "    lr_color = 0.01\n",
    "    grad_clip_norm = 1.0\n",
    "    similarity_mode = \"siglip\"\n",
    "    \n",
    "    device = \"cuda\"\n",
    "    dtype = torch.float16\n",
    "\n",
    "def opt_svg(svg, image, skip_final_optimization=False):\n",
    "    pydiffvg.set_device(torch.device(device))\n",
    "    args = OptimizationArgs\n",
    "    path_svg_original = Path(\"original.svg\")\n",
    "    path_svg_tmp = Path(\"tmp.svg\")\n",
    "    with open(path_svg_original, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(svg)\n",
    "\n",
    "    load_result = _load_svg_and_prepare_params(path_svg_original)\n",
    "    canvas_width, canvas_height, shapes, shape_groups, params = load_result\n",
    "\n",
    "    optimizer_setup = _setup_optimizer(args, params)\n",
    "    \n",
    "    optimizer, param_groups = optimizer_setup\n",
    "\n",
    "    target_image_embedding = _calculate_target_image_embedding(\n",
    "        image, siglip_model, args.device, args.dtype\n",
    "    )\n",
    "    \n",
    "    optimization_result = _run_optimization_loop(\n",
    "        args,\n",
    "        canvas_width,\n",
    "        canvas_height,\n",
    "        shapes,\n",
    "        shape_groups,\n",
    "        optimizer,\n",
    "        param_groups,\n",
    "        aesthetic_evaluator_torch=aesthetic_evaluator_torch,\n",
    "        similarity_mode=args.similarity_mode,\n",
    "        siglip_model=siglip_model,\n",
    "        target_image_embedding=target_image_embedding,\n",
    "        device=args.device,\n",
    "        dtype=args.dtype,\n",
    "        image=image\n",
    "    )\n",
    "\n",
    "    best_shapes, best_shape_groups, best_aes = optimization_result\n",
    "\n",
    "    pydiffvg.save_svg(str(path_svg_tmp), canvas_width, canvas_height, best_shapes, best_shape_groups)\n",
    "\n",
    "    if not skip_final_optimization:\n",
    "        svg_opt = optimize_svg_10k(path_svg_tmp.read_text())\n",
    "    else:\n",
    "        svg_opt = path_svg_tmp.read_text()\n",
    "    \n",
    "    return svg_opt, best_aes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b783104c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| export\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def convert_all(images, max_workers=4):\n",
    "#     svgs = []\n",
    "#     with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         for svg in tqdm(executor.map(vtracer_png_to_svg, images), total=len(images)):\n",
    "#             svgs.append(svg)\n",
    "#     return svgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfdf85e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cairosvg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "efd90bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import List, Optional\n",
    "\n",
    "# import torch\n",
    "# from diffusers import AutoPipelineForText2Image\n",
    "# from PIL import Image\n",
    "\n",
    "\n",
    "# class SDXLTurboGenerator():\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_path: str = \"stabilityai/sdxl-turbo\",\n",
    "#         guidance_scale: float = 0.0,\n",
    "#         num_inference_steps: int = 4,\n",
    "#         device: str = \"cuda\",\n",
    "#         seed: int = 42,\n",
    "#         lora_path: Optional[str] = None,\n",
    "#     ):\n",
    "#         self.model_path = model_path\n",
    "#         self.guidance_scale = guidance_scale\n",
    "#         self.num_inference_steps = num_inference_steps\n",
    "#         self.device = device\n",
    "#         self.seed = seed\n",
    "#         self.lora_path = lora_path\n",
    "#         self.pipe = self._load_pipeline()\n",
    "\n",
    "#     def _load_pipeline(self):\n",
    "#         try:\n",
    "#             pipe = AutoPipelineForText2Image.from_pretrained(\n",
    "#                 self.model_path, torch_dtype=torch.float16, variant=\"fp16\"\n",
    "#             ).to(self.device)\n",
    "#             return pipe\n",
    "#         except Exception as e:\n",
    "#             raise ValueError(f\"Failed to load pipeline: {e}\") from e\n",
    "\n",
    "#     def process(\n",
    "#         self,\n",
    "#         prompt,\n",
    "#         num_images: int = 1,\n",
    "#         negative_prompt: str = \"\",\n",
    "#         height: int = 512,\n",
    "#         width: int = 512,\n",
    "#         **kwargs,\n",
    "#     ) -> List[Image.Image]:\n",
    "#         try:\n",
    "#             generator = torch.Generator(\"cuda\")\n",
    "#             if self.seed is not None:\n",
    "#                 generator.manual_seed(self.seed)\n",
    "\n",
    "#             outputs = self.pipe(\n",
    "#                 prompt=prompt,\n",
    "#                 num_images_per_prompt=num_images,\n",
    "#                 negative_prompt=negative_prompt,\n",
    "#                 height=height,\n",
    "#                 width=width,\n",
    "#                 num_inference_steps=self.num_inference_steps,\n",
    "#                 guidance_scale=self.guidance_scale,\n",
    "#                 generator=generator\n",
    "#             )\n",
    "#             return outputs.images\n",
    "#         except Exception as e:\n",
    "#             raise RuntimeError(f\"Error in process(): {e}\") from e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef9687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator = SDXLTurboGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47530e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def gen_bitmap(prompt, seed):\n",
    "#     images = generator.process(prompt, 2)\n",
    "#     return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c9f61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict(prompt: str) -> str:\n",
    "#     time_start = time.time()\n",
    "#     original_images = []\n",
    "#     images = []\n",
    "\n",
    "#     # Generate images from text\n",
    "#     for seed in tqdm(range(4), desc=\"Generate image\"):\n",
    "#         bitmap = gen_bitmap(prompt, seed)\n",
    "#         original_images.extend(bitmap)\n",
    "\n",
    "#     # Convert bitmaps to SVGs\n",
    "#     svgs = convert_all(original_images, max_workers=4)\n",
    "\n",
    "#     for svg in svgs:\n",
    "#         # Convert SVG to raster image (for scoring)\n",
    "#         png_bytes = cairosvg.svg2png(bytestring=svg.encode('utf-8'))\n",
    "#         image = Image.open(io.BytesIO(png_bytes))\n",
    "#         images.append(image)\n",
    "\n",
    "#     # Select top-N images by score\n",
    "#     step1_n = 2\n",
    "#     _, scores = rank_images(prompt, images)\n",
    "#     scores = np.array(scores)\n",
    "#     idx_top_n = np.argsort(-scores)[:step1_n]\n",
    "#     # idx_top_n = np.arange(step1_n)  # alternative fixed selection\n",
    "\n",
    "#     print(f\"{scores=}\")\n",
    "#     print(f\"{idx_top_n=}\")\n",
    "\n",
    "#     # Quick optimization stage\n",
    "#     OptimizationArgs.iterations = 10\n",
    "#     OptimizationArgs.jpeg_iter = 10\n",
    "#     list_svgs_tmp = []\n",
    "#     list_aes_tmp = []\n",
    "#     list_images_tmp = []\n",
    "\n",
    "#     for idx in idx_top_n:\n",
    "#         svg = svgs[idx]\n",
    "#         original_image = original_images[idx]\n",
    "#         svg_tmp, aes_tmp = opt_svg(svg, original_image, skip_final_optimization=True)\n",
    "#         list_svgs_tmp.append(svg_tmp)\n",
    "#         list_aes_tmp.append(aes_tmp)\n",
    "#         list_images_tmp.append(original_image)\n",
    "\n",
    "#     bestidx = np.argmax(list_aes_tmp)\n",
    "\n",
    "#     print(f\"aes list = {list_aes_tmp}\")\n",
    "#     print(f\"{bestidx=}\")\n",
    "\n",
    "#     svg = list_svgs_tmp[bestidx]\n",
    "#     image = list_images_tmp[bestidx]\n",
    "#     image.save(\"raw_image.png\")\n",
    "\n",
    "#     # Final optimization stage\n",
    "#     OptimizationArgs.iterations = 100\n",
    "#     OptimizationArgs.jpeg_iter = 100\n",
    "\n",
    "#     \"\"\"\n",
    "#     # Alternative: use best index from initial CLIP scoring instead\n",
    "#     bestidx, scores = rank_images(prompt, images)\n",
    "#     svg = svgs[bestidx]\n",
    "#     image = original_images[bestidx]\n",
    "#     \"\"\"\n",
    "\n",
    "#     print(f\"SVG length before optimization: {len(svg.encode())}\")\n",
    "#     svg, _ = opt_svg(svg, image)\n",
    "#     print(f\"SVG length after optimization: {len(svg.encode())}\")\n",
    "\n",
    "#     time_end = time.time()\n",
    "#     time_total = time_end - time_start\n",
    "#     print(f\"{time_total=}\")\n",
    "#     return svg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0dfa5657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# svg = predict(\"flat color illustration, watercolor painting of a fire xbreathing dragon, inspired by Tom Whalen, vibrant palette, bold outlines, simple shapes, app icon.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31a96a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(svg.encode()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d8ed21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded SVG file. Canvas: 384x384\n",
      "  Parameters to optimize: points=121, stroke_width=121, color=122\n",
      "  Computed encoding for target image. Shape: torch.Size([1, 1152])\n",
      "  Starting optimization to minimize loss (Aesthetic + SIGLIP) (Batch Size: 1)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/200 [00:00<02:10,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [   1/200], LRs: 0.300000/0.000000/0.010000, AesScore(T Avg): 0.532227, SIGLIPLoss(Raw Avg): -0.854980, SIGLIPLoss(Norm/Direct Avg): -0.854980, Loss(A/Sim/Mse/T Avg): -0.5322/-0.8550/0.0171/-53.367111, Best Loss: -53.367111 (iter 0), Time: 0.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 10/200 [00:02<00:46,  4.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  10/200], LRs: 0.298653/0.000000/0.009955, AesScore(T Avg): 0.584961, SIGLIPLoss(Raw Avg): -0.886719, SIGLIPLoss(Norm/Direct Avg): -0.886719, Loss(A/Sim/Mse/T Avg): -0.5850/-0.8867/0.0162/-66.165077, Best Loss: -66.165077 (iter 9), Time: 2.84s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 21/200 [00:05<00:36,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  20/200], LRs: 0.294032/0.000000/0.009801, AesScore(T Avg): 0.609863, SIGLIPLoss(Raw Avg): -0.895508, SIGLIPLoss(Norm/Direct Avg): -0.895508, Loss(A/Sim/Mse/T Avg): -0.6099/-0.8955/0.0155/-72.963921, Best Loss: -75.453850 (iter 18), Time: 4.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 31/200 [00:06<00:31,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  30/200], LRs: 0.286234/0.000000/0.009541, AesScore(T Avg): 0.641602, SIGLIPLoss(Raw Avg): -0.898926, SIGLIPLoss(Norm/Direct Avg): -0.898926, Loss(A/Sim/Mse/T Avg): -0.6416/-0.8989/0.0149/-79.394211, Best Loss: -79.394211 (iter 29), Time: 6.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 41/200 [00:08<00:28,  5.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  40/200], LRs: 0.275450/0.000000/0.009182, AesScore(T Avg): 0.642090, SIGLIPLoss(Raw Avg): -0.899414, SIGLIPLoss(Norm/Direct Avg): -0.899414, Loss(A/Sim/Mse/T Avg): -0.6421/-0.8994/0.0144/-82.216873, Best Loss: -82.290421 (iter 38), Time: 8.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 51/200 [00:10<00:27,  5.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  50/200], LRs: 0.261947/0.000000/0.008732, AesScore(T Avg): 0.667480, SIGLIPLoss(Raw Avg): -0.900391, SIGLIPLoss(Norm/Direct Avg): -0.900391, Loss(A/Sim/Mse/T Avg): -0.6675/-0.9004/0.0142/-85.621758, Best Loss: -85.621758 (iter 49), Time: 10.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 61/200 [00:12<00:25,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  60/200], LRs: 0.246057/0.000000/0.008202, AesScore(T Avg): 0.678711, SIGLIPLoss(Raw Avg): -0.902344, SIGLIPLoss(Norm/Direct Avg): -0.902344, Loss(A/Sim/Mse/T Avg): -0.6787/-0.9023/0.0140/-88.016617, Best Loss: -88.016617 (iter 59), Time: 12.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 71/200 [00:14<00:22,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  70/200], LRs: 0.228171/0.000000/0.007606, AesScore(T Avg): 0.670410, SIGLIPLoss(Raw Avg): -0.904297, SIGLIPLoss(Norm/Direct Avg): -0.904297, Loss(A/Sim/Mse/T Avg): -0.6704/-0.9043/0.0140/-87.364685, Best Loss: -88.261024 (iter 67), Time: 14.01s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 81/200 [00:15<00:21,  5.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  80/200], LRs: 0.208729/0.000000/0.006958, AesScore(T Avg): 0.659180, SIGLIPLoss(Raw Avg): -0.896973, SIGLIPLoss(Norm/Direct Avg): -0.896973, Loss(A/Sim/Mse/T Avg): -0.6592/-0.8970/0.0137/-87.279015, Best Loss: -89.819351 (iter 74), Time: 15.76s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 91/200 [00:17<00:20,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [  90/200], LRs: 0.188210/0.000000/0.006274, AesScore(T Avg): 0.680176, SIGLIPLoss(Raw Avg): -0.902832, SIGLIPLoss(Norm/Direct Avg): -0.902832, Loss(A/Sim/Mse/T Avg): -0.6802/-0.9028/0.0138/-89.242439, Best Loss: -91.293625 (iter 88), Time: 17.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 101/200 [00:19<00:18,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 100/200], LRs: 0.167120/0.000000/0.005571, AesScore(T Avg): 0.698730, SIGLIPLoss(Raw Avg): -0.912598, SIGLIPLoss(Norm/Direct Avg): -0.912598, Loss(A/Sim/Mse/T Avg): -0.6987/-0.9126/0.0138/-92.195816, Best Loss: -92.195816 (iter 99), Time: 19.45s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 111/200 [00:21<00:16,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 110/200], LRs: 0.145978/0.000000/0.004866, AesScore(T Avg): 0.706055, SIGLIPLoss(Raw Avg): -0.911621, SIGLIPLoss(Norm/Direct Avg): -0.911621, Loss(A/Sim/Mse/T Avg): -0.7061/-0.9116/0.0136/-93.928772, Best Loss: -93.928772 (iter 109), Time: 21.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 121/200 [00:23<00:14,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 120/200], LRs: 0.125305/0.000000/0.004177, AesScore(T Avg): 0.700195, SIGLIPLoss(Raw Avg): -0.912109, SIGLIPLoss(Norm/Direct Avg): -0.912109, Loss(A/Sim/Mse/T Avg): -0.7002/-0.9121/0.0136/-93.119934, Best Loss: -94.930260 (iter 113), Time: 23.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 131/200 [00:24<00:11,  5.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 130/200], LRs: 0.105608/0.000000/0.003520, AesScore(T Avg): 0.700195, SIGLIPLoss(Raw Avg): -0.912109, SIGLIPLoss(Norm/Direct Avg): -0.912109, Loss(A/Sim/Mse/T Avg): -0.7002/-0.9121/0.0134/-94.393501, Best Loss: -95.893593 (iter 128), Time: 24.85s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 141/200 [00:26<00:10,  5.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 140/200], LRs: 0.087374/0.000000/0.002912, AesScore(T Avg): 0.684570, SIGLIPLoss(Raw Avg): -0.906738, SIGLIPLoss(Norm/Direct Avg): -0.906738, Loss(A/Sim/Mse/T Avg): -0.6846/-0.9067/0.0131/-93.482246, Best Loss: -95.893593 (iter 128), Time: 26.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 151/200 [00:28<00:08,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 150/200], LRs: 0.071052/0.000000/0.002368, AesScore(T Avg): 0.708008, SIGLIPLoss(Raw Avg): -0.914062, SIGLIPLoss(Norm/Direct Avg): -0.914062, Loss(A/Sim/Mse/T Avg): -0.7080/-0.9141/0.0133/-95.771240, Best Loss: -96.040894 (iter 143), Time: 28.48s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 161/200 [00:30<00:06,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 160/200], LRs: 0.057043/0.000000/0.001901, AesScore(T Avg): 0.683105, SIGLIPLoss(Raw Avg): -0.909180, SIGLIPLoss(Norm/Direct Avg): -0.909180, Loss(A/Sim/Mse/T Avg): -0.6831/-0.9092/0.0133/-92.623894, Best Loss: -97.401848 (iter 150), Time: 30.25s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 171/200 [00:32<00:05,  5.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 170/200], LRs: 0.045692/0.000000/0.001523, AesScore(T Avg): 0.692383, SIGLIPLoss(Raw Avg): -0.900879, SIGLIPLoss(Norm/Direct Avg): -0.900879, Loss(A/Sim/Mse/T Avg): -0.6924/-0.9009/0.0131/-93.870361, Best Loss: -97.401848 (iter 150), Time: 32.08s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [00:33<00:03,  5.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 180/200], LRs: 0.037278/0.000000/0.001243, AesScore(T Avg): 0.716309, SIGLIPLoss(Raw Avg): -0.914551, SIGLIPLoss(Norm/Direct Avg): -0.914551, Loss(A/Sim/Mse/T Avg): -0.7163/-0.9146/0.0135/-95.592072, Best Loss: -97.401848 (iter 150), Time: 33.91s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 191/200 [00:35<00:01,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 190/200], LRs: 0.032010/0.000000/0.001067, AesScore(T Avg): 0.699707, SIGLIPLoss(Raw Avg): -0.910645, SIGLIPLoss(Norm/Direct Avg): -0.910645, Loss(A/Sim/Mse/T Avg): -0.6997/-0.9106/0.0131/-95.382980, Best Loss: -97.648201 (iter 183), Time: 35.70s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:37<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Iter [ 200/200], LRs: 0.030017/0.000000/0.001001, AesScore(T Avg): 0.706055, SIGLIPLoss(Raw Avg): -0.913574, SIGLIPLoss(Norm/Direct Avg): -0.913574, Loss(A/Sim/Mse/T Avg): -0.7061/-0.9136/0.0133/-95.327522, Best Loss: -97.873169 (iter 194), Time: 37.44s\n",
      "  Optimization completed! Best Loss: -97.873169 at iteration 194\n",
      "Resized to 281\n"
     ]
    }
   ],
   "source": [
    "image = Image.open(\"/home/anhndt/pysvgenius/notebooks/raw_image.png\")\n",
    "with open(\"/home/anhndt/pysvgenius/notebooks/original.svg\", \"r\") as f:\n",
    "    svg = f.read()\n",
    "    \n",
    "svg, _ = opt_svg(svg, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a601f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysvgenius",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
